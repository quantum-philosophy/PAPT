In this section, numerical results of the proposed framework for the illustrative example given in \sref{illustrative-example} are presented. All the experiments are implemented in MATLAB R2012a \cite{matlab} and conducted in \hostOS\ running on \hostHardware.

The effective channel length $\Leff$ is assumed to deviate by $5\%$ from the nominal value of $45$ nm where the global and local variations are equally weighted \cite{juan2011, juan2012}. Correlation matrices are computed according to \eref{correlation-matrix}, in which the correlation length $\corrLength$ is half the size of the die. In the model order reduction technique, described in \sref{preprocessing-rvs}, the threshold parameter $\Lth$ is set to $0.99$, which makes the preserved eigenvalues capture $99\%$ of the variance of the data. Dynamic power profiles involved in the experiments are based on simulations of synthetic applications on hypothetical platforms. The floorplans of the platforms are generated in such a way that the processing elements are placed in symmetric grids, as it is the case, for instance, with Alpha 21264 studied in \cite{juan2011}. Time steps of power and, consequently, temperature traces are set to one millisecond, i.e., $\dt_i = 10^{-3}$s, $\forall i$. The reference leakage current $I_\leak(\Leff, \T)$, obtained via SPICE simulations (see \sref{power-model-construction}), is scaled up to the power level of the processing elements in such a way that the leakage power accounts for about $40\%$ of the total power dissipation at high temperatures \cite{liu2007}. In order to construct equivalent thermal RC circuits, the temperature simulator HotSpot v5.02 \cite{hotspot} with default settings is employed. In this case, thermal packages are modeled with three layers, and the relation between the number of processing elements and the number of thermal nodes is given according to $\nodes = 4 \cores + 12$. To assess the performance of our framework, we employ a Monte Carlo (MC) sampling technique. The MC approach consists in solving \eref{fourier-original} numerically using the Dormand-Prince method, which is based on the fourth- and fifth-order Runge-Kutta formulas \cite{press2007}. The leakage model for the MC sampling is the same as for the proposed framework except the fact that the model order reduction is reasonably omitted.

\input{include/tables/accuracy-expectation.tex}
\input{include/tables/accuracy-variance.tex}
\input{include/tables/accuracy-pdf.tex}
The first set of experiments is aimed to identify the accuracy of both competitors and to find appropriate values of the polynomial order $\pcorder$ for the PC expansion and the number of samples $\mcsamples$ for the MC technique. To this end, three accuracy measures have been chosen. The first two are the normalized root mean square errors (NRMSE) of expectation and variance of the resulting temperature traces. The third measure is the mean of NRMSEs of empirical probability density functions constructed at each time step for the PC and MC approaches. The measures are respectively denoted by $\eExp$, $\eVar$, and $\ePDF$. The comparison for a quad-core architecture with a dynamic power profile of $10^2$ steps is given in \tref{accuracy-expectation}, \tref{accuracy-variance}, and \tref{accuracy-pdf} where $\pcorder$ is swept from $1$ to $5$ and $\mcsamples$ from $10^2$ to $10^5$. The veracity of the results increases from the top-left to the bottom-right corners of the tables; therefore, the last row can be viewed as an indicator of the MC error while the last column shows the error of the PC expansion. It can be seen that deviations of the expected value of both techniques are relatively small even for low precision requirements; for instance, $\eExp$ of the MC-based technique with $10^2$ samples is $0.23\%$, and $\eExp$ of the first-order PC expansion is $0.21\%$. However, this is not the case for the other two accuracy measures. $\eVar$ starts from about $30\%$ for $10^2$ MC samples and from $36\%$ for the first-order PC expansion; the error drops significantly to less than $2\%$ for $10^4$ MC samples and the third-order PC. The results of the third error estimate, $\ePDF$, are of a particular importance since they allow us to conclude that the probability density functions computed by the fourth-order (and higher) PC expansions are closely following those of the MC technique with large numbers of samples. Guided by the observations, we let the MC-based approach with $\mcsamples$ equal to $10^4$ be the etalon for our experiments and fix the polynomial order $\pcorder$ to four. In this case, we state that the error of the proposed framework is bounded by less than $0.1\%$, $2\%$, and $5\%$ for the expected value, variance, and probability density function, respectively.

\input{include/tables/scaling-cores.tex}
Now, we exercise the number of processing elements $\cores$ and, consequently, the number of stochastic dimensions $\vars$. In these experiments, the number of steps $\steps$ is constant and equal to $10^3$. The results are given in \tref{scaling-cores} along with the number of r.v.'s left after the PCA (see \sref{preprocessing-rvs}). It can be seen that the typical symmetric placement and the natural radial structure of correlation, cf. \cite{cheng2011}, open a great possibility for the model order reduction. One can also clearly observe the well-known fact that the workload per one MC sample is independent of the number of stochastic dimensions \cite{maitre2010}. At the same time, the PC demonstrates a polynomial growth \cite{heiss2008} of the computation time, which is also predictable. However, even in high dimensions, the proposed framework significantly outperforms the MC sampling. For instance, to quantify a power profile with $10^3$ steps of a multiprocessor system with $32$ cores, the MC approach requires more than $40$ hours whereas the proposed framework takes less than a minute. It should be noted separately that the number of MC samples is constant in this setup; however, in order to maintain the same accuracy, the number of samples should be increased whenever $\vars$ is increased.

\input{include/tables/scaling-steps.tex}
Finally, we investigate the scaling properties of the proposed framework with respect to the number of steps $\steps$ in the nominal dynamic power profile $\prof{\mP_\dyn}$, which is directly proportional to the simulated time. The results for a quad-core architecture are given in \tref{scaling-steps}. Due to the long computation time demonstrated by the MC approach, its data for high values of $\steps$ are extrapolated based on a few samples. It can be seen that both methods scale linearly, which is expected. However, the proposed framework shows a superior performance being more around $8$--$26$ times faster than \emph{only one sample} of the MC technique.
