In this section, numerical results of the proposed framework for the illustrative example given in \sref{illustrative-example} are presented. All the experiments are implemented in MATLAB R2012a \cite{matlab} and conducted in \hostOS\ running on \hostHardware.

The effective channel length $\Leff$ is assumend to deviate by $5\%$ from the nominal value of $45$ nm where the global and local variations are equaly weighted \cite{juan2012}. Correlation matrices are computed according to \eref{correlation-matrix}, in which the correlation length $\corrLength$ is half the size of the die. The proposed framework is set up to construct Hermite PC expansions using non-intrusive Galerkin projections based on the Smolyak algorithm for multidimensional integration. In the model reduction technique, described in \sref{preprocessing-rvs}, the threshold parameter $\Lth$ is set to $0.99$, which makes the preserved eigenvalues capture $99\%$ of the variance of the data.

Dynamic power profiles involved in the experiments are based on simulations of synthetic applications on hypothetical platforms. Time steps of power and, consequently, temperature traces are set to one millisecond, i.e., $\dt_i = 10^{-3}$s, $\forall i$. The reference leakage current $I_\leak(\Leff, \T)$, obtained via SPICE simulations (see \sref{power-model-construction}), is scaled up to the power level of the processing elements in such a way that the leakage power accounts for about $40\%$ of the total power dissipation at high temperatures \cite{liu2007}. In order to construct equivalent thermal RC circuits, the temperature simulator HotSpot v5.02 \cite{hotspot} with default settings is employed. In this case, thermal packages are modeled with three layers, and the relation between the number of processing elements and the number of thermal nodes is given according to $\nodes = 4 \cores + 12$.

To assess the performance of our framework, we employ a Monte Carlo (MC) sampling technique. The MC approach consists in solving \eref{fourier-original} numerically using the Dormand-Prince method, which is based on the fourth- and fifth-order Runge-Kutta formulas \cite{press2007}. The leakage model for the MC sampling is the same as for the proposed framework except the fact that the model reduction is reasonably omitted.

\input{include/tables/accuracy-expectation.tex}
\input{include/tables/accuracy-variance.tex}
The first set of experiments is aimed to identify the accuracy of both competitors and to find appropriate values of the polynomial order $\pcorder$ and the number of samples $\mcsamples$ for the PC expansion and MC sampling, respectively. The accuracy measure is chosen to be the normalized root mean square error (NRMSE) of expectation and variance, denoted by $\err{\sExp}$ and $\err{\sVar}$, respectively, of the resulting temperature traces. The comparison for a quad-core architecture with a dynamic power profile of $10^2$ steps is given in \tref{accuracy-expectation} for $\err{\sExp}$ and in \tref{accuracy-variance} for $\err{\sVar}$ where we vary $\pcorder$ from $1$ to $6$ and $\mcsamples$ from $10^2$ to $2 \times 10^4$. The veracity of the results increases from the top-left to the bottom-right corners of the tables, therefore, the last row can be viewed as an indicator of the MC error while the last column shows the error of the PC expansion. It can be seen that deviations of the expected value of both techniques are relatively small even for low precision requirements; for instance, $\err{\sExp}$ of the MC-based technique with $10^2$ samples is $2\%$, and $\err{\sExp}$ of the first-order PC expansion is only $0.81\%$. However, this is not the case with variance. The NRMSE for the MC approach starts from a large value of $186\%$ for $10^2$ samples and drops to $11\%$ for $10^3$ samples. The former is an outlier since $10^2$ samples are by far not sufficient for a MC technique. The later is a more realistic estimate, after which the MC sampling demonstrates its inherently slow rate of convergence \cite{xiu2009, maitre2010}. Even with $10^4$ samples, the different between the MC technique and the PC expansion is about $10\%$, and this difference is decreasing with the increase of $\mcsamples$, which means that the variance computed by the MC sampling converges to one of the PC expansion. Assuming that one can trust the statistics of $2 \times 10^4$ MC samples, we conclude that the error of the proposed framework is bounded by $1\%$ for expectation and by $5\%$ for variance for PC expansions of orders starting from four. Guided by the observations above, for the following experiments, we fix the number of samples $\mcsamples$ of the MC approach to $10^4$, which is typically consudered to be the lower bound, cf. \cite{xiu2009, bhardwaj2006}, and the polynomial order $\pcorder$ of the proposed framework to four.

\input{include/tables/scaling-steps.tex}
Now, we investigate the scaling properties of the proposed framework with respect to the number of steps $\steps$ in the (nominal) dynamic power profile $\prof{\mP_\dyn}$, which is directly proportional to simulated time. The results for a quad-core architecture are given in \tref{scaling-steps}. Due to the long computation time demonstrated by the MC approach, its data for high values of $\steps$ are extrapolated based on a few samples. It can be seen that both methods scale linearly, which is expected. However, the proposed framework shows a superior performance being more around $10$--$40$ times faster than \emph{only one sample} of the MC technique.

\input{include/tables/scaling-cores.tex}
The next parameter to exercise is the number of processing elements $\cores$ and, consequently, the number of stochastic dimensions $\vars$. In these experiments, the number of steps $\steps$ is constant and equal to $10^3$. The results are given in \tref{scaling-cores}. From the table, we can clearly see the well-known fact that the workload per one MC sample is independent of the number of stochastic dimensions \cite{maitre2010}. At the same time, we observe a polynomial growth \cite{heiss2008} of the PC expansion based on sparse grids, which is also predictable. However, even in high dimensions, the proposed framework significantly outperforms the MC sampling. For instance, to quantify a power profile with $10^3$ steps of a multiprocessor system with 32 cores, the MCS requires more than 35 hours whereas the proposed framework takes only 8 minutes. It should be noted separately that the number of samples of the MCS is constant in this setup; however, in order to maintain the same accuracy, the number of samples should be increased whenever $\vars$ is increased [?].
