In this section, numerical results of the proposed framework for the illustrative example in \sref{illustrative-example} are reported. All the experiments are implemented in MATLAB R2012a \cite{matlab} and conducted in \hostOS\ running on \hostHardware.

The channel length $\Leff$ is assumed to deviate by $5\%$ from the nominal value of $45$ nm where the global and local variations are equally weighted \cite{juan2011, juan2012}. Correlation matrices are computed according to \eref{correlation-matrix}, in which the correlation length $\corrLength$ is half the size of the die. In the model order reduction technique (see \sref{ie-uncertain-parameters}) the threshold parameter $\Lth$ is set to $0.99$ preserving $99\%$ of the variance of the data. Dynamic power profiles involved in the experiments are based on simulations of randomly generated applications defined as task graphs.\footnote{The task graphs of the applications, floorplans of the platforms, configuration of the temperature simulator used here are available online at \cite{sources}.} The floorplans of the platforms are constructed in such a way that the processing elements are placed in symmetric grids, as it is the case with, e.g., Alpha 21264 studied in \cite{juan2011}. Time steps of power and temperature traces are set to one millisecond, i.e., $\dt_i = 10^{-3}$s, $\forall i$ (see \sref{problem-formulation}). The reference leakage current $I_\leak(\Leff, \T)$ (see \sref{ie-power-model}) is scaled to accounts for about $40\%$ of the total power at high temperatures \cite{liu2007}. To assess the performance of our framework, we employ a Monte Carlo (MC) sampling technique. The MC approach consists in solving \eref{fourier-original} numerically using the Dormand-Prince method, which is based on the fourth- and fifth-order Runge-Kutta formulas \cite{press2007}. Based on theoretical estimations \cite{diaz-emparanza2002} of the accuracy of MC simulations, experience from the literature \cite{xiu2010, eldred2009, maitre2010, shen2009}, and our observations, we let the MC approach with $10^4$ samples be the etalon for our benchmarks.

\input{include/assets/accuracy.tex}
The first set of experiments is aimed to identify the accuracy of our framework and, consequently, to find a reasonable value of the polynomial order $\pcorder$. To this end, three accuracy metrics have been chosen. The first two are the normalized root mean square errors (NRMSE) of expectation and variance of the resulting temperature traces. The third metric is the mean of NRMSEs of empirical probability density functions constructed at each time step for each processing element. The comparison for a quad-core architecture with a dynamic power profile of $10^2$ steps is given in \fref{accuracy}, where $\pcorder$ is swept from $1$ to $5$. It can be seen that deviations of the expected value is relatively small even for low precision requirements: the error of the first-order PC expansion is bounded by a fraction of one percent. The NRMSE of variance starts from $36\%$ for the first-order PC expansion and drops significantly to less than $2\%$ for the third-order PC. The result of the third error estimate is of a particular importance since it allows us to conclude that the probability density functions computed by the fourth-order (and higher) PC expansions are closely following those of the MC technique with a large number of samples. Guided by the above observations, we fix the polynomial order $\pcorder$ to four for the rest of the experiments and state that the error of our technique is bounded by less than $0.2\%$ for expectation and less than $2\%$ for variance and probability density functions.

\input{include/assets/speed.tex}
Now, we focus on the computational speed. First, we vary the number of processing elements $\cores$ and, consequently, the number of \rvs\ $\vars$. In these experiments, the number of steps $\steps$ is constant and equal to $10^3$. The results are given in \tref{scaling-cores} along with the number of \rvs\ left after the PCA (see \sref{ie-uncertain-parameters}). It can be seen that the typical symmetric placement and natural radial structure of correlations \cite{cheng2011} open a great possibility of model order reduction. One can also clearly observe the well-known fact that the workload per one MC sample is independent of the number of stochastic dimensions \cite{maitre2010}. At the same time, the PC demonstrates a polynomial growth \cite{heiss2008} of the computation time. However, even in high dimensions, the proposed framework significantly outperforms the MC sampling. For instance, to quantify a power profile with $10^3$ steps of a multiprocessor system with $32$ cores, the MC approach requires more than $40$ hours whereas the proposed framework takes less than 10 seconds.

Finally, we investigate the scaling properties of the proposed framework with respect to the number of steps $\steps$ in the nominal dynamic power profile $\profPdyn$, which is directly proportional to the simulated time. The results for a quad-core architecture are given in \tref{scaling-steps}. Due to the long computation time demonstrated by the MC approach, its data for high values of $\steps$ are extrapolated based on a few samples. It can be seen that both methods scale linearly. However, the proposed framework shows a superior performance.
