To be legitimate in the context of probability theory, we need to start by introducing the probability space that we shall reside in. This space is defined as a triple $(\outcomes, \sAlgebra, \pMeasure)$ where $\outcomes$ is a set of outcomes, $\sAlgebra \subseteq 2^\outcomes$ is a $\sigma$-algebra on $\outcomes$, and $\pMeasure: \sAlgebra \to [0, 1]$ is a probability measure \cite{durrett2010}. Loosely speaking, an $n$-dimensional \rv\ is then a mapping $\v{X}: \o \in \outcomes \mapsto \v{X}(\o) \in \real^n$, and a random process is an infinite set of \rvs. In what follows, the space $(\outcomes, \sAlgebra, \pMeasure)$ is always implied.

As introduced earlier, we adapt the theory of PC expansions in order to construct a polynomial approximation to the initial problem. Mutual independence and finiteness of the uncertain parameters $\vU(\o)$ are essential prerequisites of PC. In general, however, $\vU(\o)$ can be given as a stochastic process or as a finite but correlated set of \rvs. In such cases, $\vU(\o)$ should be preprocessed in order to fulfill the requirement of PC. To this end, an appropriate probability transformation should be undertaken. Denote such a transformation by $\vU(\o) = \oTransform{\vZ(\o)}$, which relates the initial uncertain parameters $\vU(\o)$ with a set of $\vars$ mutually independent \rvs\ $\vZ(\o)$. Without loss of generality, $\vZ(\o)$ are assumed to be centered and normalized, \ie, $\oExp{\vZ(\o)} = \mZero$ and $\oCov{\vZ(\o)} = \mI$. In the rest of the subsection, we outline some of the most typical scenarios of $\vU(\o)$ with the corresponding transformations and refer the reader to \cite{xiu2010, eldred2009} for in-depth discussions.

Assume the uncertainty $\vU(\o)$ is given as a continuous-space stochastic process $\vU(r, \o)$, for some spacial parameter $r$, with a prescribed covariance function \cite{durrett2010}. The case is commonly addressed by a spectral decomposition known as the Karhunen-Lo\`{e}ve (KL) expansion \cite{xiu2010, maitre2010, ghanem1991}. Based on the covariance function of the process, KL factorizes $\vU(r, \o)$ into a summation of orthogonal \rvs\ weighted by deterministic functions of $r$. Such a Fourier-like series is then truncated, and the result is a finite set of uncorrelated \rvs\ that are typically also assumed to be independent; however, the assumption formally holds only for Gaussian processes. When correlations are strong, KL becomes especially efficient as the number of \rvs\ to preserve can be substantially small, yet sufficient to retain accuracy. The technique is applied in \sref{ie-uncertain-parameters} and is explained in \aref{uncertain-parameters}.

Assume $\vU(\o)$ is a finite set of \rvs\ with a given covariance matrix. KL is then reduces to the well-known principal component analysis (PCA), singular value decomposition (SVD), or eigenvalue decomposition---they are all the same---of the covariance matrix. Therefore, the variables can be decorrelated and reduced in number; however, as before, the result is formally independent if $\vU(\o)$ is a Gaussian vector.

Assume $\vU(\o)$ is a finite set of \rvs, and it does not follow a Gaussian distribution, and the assumption of independence is not affordable. In this case, the most prominent solutions are the Rosenblatt and Nataf transformations \cite{eldred2009, li2008}. Rosenblatt's approach is suitable when the joint \pdf\ of $\vU(\o)$ is known; however, such information is rarely available. The marginal distributions and correlation matrix of $\vU(\o)$ are more likely to be given, which are already sufficient to perform the Nataf transformation. The latter is also straightforward for implementation; see \cite{li2008}.
