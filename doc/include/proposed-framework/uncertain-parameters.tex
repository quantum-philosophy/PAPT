To be legitimate in the context of probability theory, we need to formally define the notion of a probability space and a \rv\ A probability space  \cite{maitre2010} is defined as a triple $(\outcomes, \sAlgebra, \pMeasure)$ where $\outcomes$ is a set of outcomes, $\o \in \outcomes$, $\sAlgebra \subseteq 2^\outcomes$ is a $\sigma$-algebra on $\outcomes$, and $\pMeasure: \sAlgebra \to [0, 1]$ is a probability measure. An $n$-dimensional \rv\ is a mapping $\v{X}: \o \in \outcomes \mapsto \v{X}(\o) \in \real^n$. Hereafter, all the derivations and conclusions are made with respect to $(\outcomes, \sAlgebra, \pMeasure)$.

Mutual independence and finiteness of the set of uncertain parameters $\vU(\o)$ are essential prerequisites of PC. In general, however, $\vU(\o)$ can be given as (a) a random process, \ie, an infinite collection of \rvs, with a prescribed covariance kernel; or (b) as a finite but correlated set of \rvs. In these cases, $\vU(\o)$ should be preprocessed in order to fulfill the conditions. To this end, an adequate probability transformation should be undertaken. Denote such a transformation by $\vU(\o) \approx \oTransform{\vZ(\o)}$, which relates the initial uncertain parameters $\vU(\o)$ with a set of $\vars$ mutually independent \rvs\ $\vZ(\o)$. Without loss of generality, $\vZ(\o)$ are assumed to be centered and normalized, \ie, $\oExp{\vZ(\o)} = \mZero$ and $\oCov{\vZ(\o)} = \mI$. Note that the transformation, in general, is an approximation. In the following paragraph, we outline a possible strategy of choosing a suitable transformation and refer the reader to \cite{xiu2010, eldred2009} for in-depth discussions.

Random processes with prescribed covariance kernels are commonly addressed by the KL expansion \cite{xiu2010, maitre2010, ghanem1991}. KL decomposes the covariance kernel of a stochastic process into a set of eigenfunctions and the corresponding eigenvalues and, sequentially, expands the process into a Fourier-like series, which is further being truncated. The result is a finite set of orthogonal \rvs\ that are typically assumed to be independent; however, the assumption formally holds only for Gaussian kernels. In the finite-dimensional case, \ie, $\vU(\o)$ is a vector, KL reduces to the well-known principal component analysis (PCA), singular value decomposition (SVD), or eigenvalue decomposition---they are all the same---of the covariance matrix of $\vU(\o)$; the technique is demonstrated in \sref{ie-uncertain-parameters}. When a finite set of \rvs\ does not follow a normal distribution, it makes sense to look at other althernatives. The most prominent examples are the Rosenblatt and Nataf transformations \cite{eldred2009}. Rosenblatt's approach is suitable when the joint probability distribution function of $\vU(\o)$ is known; however, such information is rarely available. The marginal distributions and correlation matrix of $\vU(\o)$ are more likely to be given, which are sufficient to perform the Nataf transformation; besides, the latter is rather straightforward to be implemented (see \cite{li2008}).
