\begin{reviewer}
Summary:

The paper presents an approach to analyze the power and temperature of a system under process variations. The approach is based on modeling the power and temperature at any point in time using a polynomial chaos expansion (PCE). The paper is organized well and covers the relevant details of the methods used such as details of the PCE, Smolyak Quadrature etc. in the appendix while not moving away from the main focus of the paper.
\end{reviewer}
\begin{authors}
Thank you.
\end{authors}

\begin{reviewer}
Still, the paper could be further strengthened by clearing up a few things as described below.

\clabel{3}{1}
The authors try to motivate the subject of variational analysis by mentioning that the design at nominal parameters does not guarantee that it will work under severe conditions. This is, however, quite well known and for these reasons no one designs at nominal corners and end up putting up a big enough guard band to design the system considering severe conditions. Thus, they end up being quite conservative and over-designing their system. This is where the variational analysis comes into picture to alleviate the problem of over-designing by allowing the design a better estimate of how the severe conditions impact the performance/power/temperature of the system.
\end{reviewer}
\begin{authors}
It is a very good point, which is now included in the paper.
We shall also use it to respond to \cref{5}{3}.

\begin{actions}
  \action{The motivation behind this work has been extended in Sec.~I.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{3}{2}
Section VI: The reviewer likes the use of the correlation function that captures both the correlations between the devices close to each other and also equidistant from the center of the die.
\end{reviewer}
\begin{authors}
Thank you.
\end{authors}

\begin{reviewer}
\clabel{3}{3}
Section VI-A:  Some discussion on how the Nataf transformation compares with Independent component analysis which works even on non-Gaussian random variables. Some discussion on when Nataf transformation is valid or when does its accuracy suffers and what are the properties of the distributions that can be well transformed using this transformation will be useful to the reader.
\end{reviewer}
\begin{authors}
Thank you for drawing our attention to ICA; we have mentioned this technique as a potential alternative in the revised version of the manuscript.

Since probability transformations were not the focal point of the paper, we tried to avoid too involved discussions in Sec.~V-A and to give the reader a big picture instead.
As any other method, the Nataf transformation is not universal and works best only under certain conditions.
More precisely, the strict formulation requires the copula of the distribution to be elliptical.
However, in reality, the joint probability distribution is nearly never known [Li, 2008], and, therefore, the requirement cannot be verified beforehand.
Regardless of whether this requirement is fulfilled, the transformation is typically viewed as an approximation that allows one to tackle real-world problems without requiring the knowledge of the joint probability distribution [Eldred, 2008].
It should also be noted right away that the inputs to the Nataf transformations, \ie, a covariance matrix and a set of marginal probability distributions, are already not sufficient to fully specify a joint probability distribution; hence, in general, it \emph{is} an approximation.

\begin{actions}
  \action{The elliptical-copula assumption of the Nataf transformation has been included in Sec.~V-A}.
  \action{It has been clarified that the two transformations mentioned in Sec.~V-A are only two possible alternatives for the extraction of independent parameters.}
  \action{Independent component analysis has also been mentioned in Sec.~V-A.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{3}{4}
Leakage Analysis: Leakage is an exponential function of gate length, this kind of shows up in the experimental results section where the authors see that the errors in the PC expansion reduce with the expansion order for 4-5 or more. The following previous approach for leakage analysis considers the leakage as exponential function of gate length and approximates the leakage as an exponential function of the PC expansion in gate length. Such exponential model will be more accurate at lower orders of expansion and still afford similar efficiency of computation as the normal PC expansion. It will be good if the authors can compare and contrast with the exponential modeling as described in:

S. Bhardwaj et.al. ``A Unified approach for Full chip statistical timing and leakage analysis of nanoscale circuits considering intra-die process variations''.
\end{reviewer}
\begin{authors}
Thank you for the reference; it has been reviewed and included in the revised manuscript.

The reviewer is right that, if we were to focus solely on the variability of the effective channel length, it would be worth considering some ad hoc solutions, like the exponentiation of PC expansions.
However, in this case, the user of the proposed framework would need to find such an ad hoc solution, \ie, to adapt the framework, for every parameter or every combination of parameters, which is relevant to his/her problem, individually.
It can also be seen right in [Bhardwaj, 2008], mentioned by the reviewer, that even the effective channel length does not naturally fit into the exponential model.
The reason is that, according to the BSIM4 model, the inverse of the effective channel length appears in front of the exponent in the expression for the subthreshold leakage current.
To tackle the difficulty, [Bhardwaj, 2008] pulls this ``inconvenient'' term inside the exponent using the usual exp/log transformation, as shown in App.~A in that paper.
Regardless of where the effective channel length ends up in the formula, the essence of the physical process behind stays unchanged.
Our proposed approach eliminates the need of such manipulations.
It is very flexible in this regard and can be applied in various situations in a straightforward manner.

Moreover, as we have already mentioned, our primary target is temperature, and such ad hoc solutions as the exponentiation of PC expansions would hinder the achievement of this target.
More precisely, as it is summarized at the end of Sec.~V-B, the recurrent expression in Eq.~6 is linear in terms of the base polynomials, and this linearity property is highly beneficial from the computational perspective, leading to the recurrence in Eq.~7.
The property would be lost if we had one PC expansion next to an exponent of another PC expansion, for example.
At this point, one should not forget the following two facts inherent to the modeled physical phenomenon, \ie, heat transfer within electronic systems: First, the aforementioned recurrent expressions operate with vectors and matrices, which basically means that the temperature of one thermal node affects the temperatures of all other nodes.
Second, since the leakage power and temperature are tightly interrelated, at each step of the iterative process, the expansion of power is based on the previous expansion of temperature, and this dependency is strictly nonlinear (it is also emphasized at the end of Sec.~V-D3).
Consequently, stochastic temperature analysis significantly differs from stochastic power analysis and, thus, requires a different mindset in order to solve the problem efficiently.

To summarize, taking into consideration the above discussion, such a comparison (\ie, our PC expansions for power \vs\ the exponentiated PC expansions for power in [Bhardwaj, 2008]) is not possible since our algorithm is designed with temperature in mind and does not allow for a substitution of an ad hoc expression for power.

\begin{actions}
  \action{The above discussion has been appended to Sec.~V-D.}
  \action{The absence of a comparison for power has been explained in Sec.~VII.}
  \action{The paper [Bhardwaj, 2008] has been reviewed and cited.}
\end{actions}
\end{authors}

\begin{reviewer}
Experimental Results:

\clabel{3}{5}
It is somewhat unclear how much variation in the parameters was assumed. The authors mention that the channel length was assumed to deviate by 5\%. So, does it mean that the standard deviation was 5\% or 3-sigma was 5\%. If it is the latter (3-sigma = 5\%), then the variation assumption in gate length is somewhat small. We need to see how the accuracy of the approach at some higher degrees of variation. So, some comparison with 10-15\% 3-sigma variation will be important.
\end{reviewer}
\begin{authors}
We apologize for the confusion.
By ``assumed to deviate by 5\% from the nominal value,'' we meant ``assumed to have a standard deviation of 5\% of the nominal value.''
So, 5\% is about $\sigma$, not $3 \sigma$.

\begin{actions}
  \action{The formulation regarding the variability of the effective channel length has been updated at the beginning of Sec.~VII.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{3}{6}
Although PC expansion is a really good tool to estimate the system response in the presence of process variations, the authors only consider a single source of variations, i.e. gate length, and hence the error estimates and the runtimes shown will be somewhat optimistic compared to the scenario where there are more sources of variations for example: threshold voltage, oxide thickness etc. For this reason, the reviewer suggests highlighting some of the discussion related to the curse of dimensionality in the main section in addition to discussing it in the Appendix.
\end{reviewer}
\begin{authors}
We agree with the reviewer that the more uncertain parameters are taken into account, the more independent variables might eventually be needed for the construction of a sufficiently accurate PC expansion (see our answer to the seventh comment below).
This, of course, can intensify the curse of dimensionality.
The concern is described in one of the main sections of the manuscript, namely, in Sec.~V-D4, in which we also summarize how we address this issue.

In response to \cref{4}{1}, we have also included an analysis of the time complexity of the PC expansions performed inside our proposed framework, Sec.~V-D4.

With regard to the choice of the unique uncertain parameter for the illustration example, the reviewer might also read our answer to \cref{5}{4}.

\begin{actions}
  \action{The curse of dimensionality has been further elaborated on in Sec.~V-D4.}
\end{actions}
\end{authors}

\begin{reviewer}
7. For practical purposes, it would be nice to see how the convergence rates and the computational speeds change with additional sources of variations such as the threshold voltage. Some commentary on this will help readers better understand the applicability of this approach.
\end{reviewer}
\begin{authors}
Let us note the following two aspects:

First, as the reviewer correctly pointed out earlier, the curse of dimensionality constitutes a concern, which is arguably the major one when it comes to PC expansions.
Therefore, the applicability of the proposed framework mainly depends on how this curse of dimensionality manifests itself in the problem at hand.
This manifestation, in its turn, depends primarily on the number $\nvars$ of independent parameters $\vZ(\o)$ left after the preprocessing stage described in Sec.~V-A; this fact is explained in Sec.~V-D4, ``Computational challenges.''

Second, we would like to argue that conclusions about the performance of the proposed framework, which are based on the number/nature of the considered sources of variations $\vU(\o)$, are misleading.
Suppose, apart from the effective channel length $L$, we take into account one additional parameter denoted by $W$.
Then, the first question to ask is whether $L$ and $W$ are independent.
Assume first they are.
The second question to ask is whether $W$ has local variations.
If it does not, $\nvars$ will be increased by one, relative to the case wherein only $L$ is considered.
If it does, the third question to ask is: What is the correlation patter inherent to $W$?
For instance, if the pattern is the same as for $L$, $\nvars$ will be doubled in the end.
In general, the answer depends on how strong/weak the assumed spatial correlations of $W$ are.
Coming back to the very first question we asked, suppose $L$ and $W$ are now correlated.
In this case, the follow-up questions are the same as before; however, $\nvars$ will generally be smaller than in the first scenario due to additional possibilities for model order reduction.
As we can see, it is not straightforward to relate the considered uncertain parameters $\vU(\o)$ with the resulting independent random variables $\vZ(\o)$ and, thus, with the complexity of the further analysis.

To summarize, we believe that the foremost quantity to pay attention to is the dimensionality of $\vZ(\o)$, $\nvars$, which, of course, can increase with additional uncertain parameters.
The column of Tab.~V titled ``$\nvars$'' serves this exact purpose: it demonstrates how the computational speed of the proposed framework scales with respect to the number of independent parameters.
The fact that there is another variable in Tab.~V, namely, the number of processing elements, $\nprocs$, does not affect the corresponding conclusions since, as mentioned above, the complexity is dictated by the stochastic dimensionality of the problem.
Regarding accuracy, we observed that fourth-order PC expansions (used to obtain the results in Tab.~V) were sufficiently accurate for the purpose of temperature analysis considered in this paper.
Note also that this order is higher than the one used for power in the PC-related literature discussed in Sec.~II: typically, it is the second order (see, \eg, [Bhardwaj, 2008] and [Shen, 2009]).
Therefore, following a similar reasoning as in the previous paragraph, the information provided by Tab.~V allows the reader to draw conclusions about the performance of the proposed framework when additional parameters are considered.

Let us also comment on the threshold voltage mentioned by the reviewer.
It is a function of the effective channel length according to the BSIM4 model, which is also noted in the manuscript.
The dependency of the threshold voltage to the effective channel length is one of the strongest [Shen, 2009] [Juan, 2011, 2012].
Consequently, by considering the variability of the latter, we implicitly take into consideration one of the largest portions of the variability of the former.

As with the previous comment, our answer to \cref{5}{4} regarding the considered uncertain parameters is relevant to the current comment.

\begin{actions}
  \action{The above discussion has been included in Sec.~VII-B.}
\end{actions}
\end{authors}
