\begin{reviewer}
Summary:

The paper presents an approach to analyze the power and temperature of a system under process variations. The approach is based on modeling the power and temperature at any point in time using a polynomial chaos expansion (PCE). The paper is organized well and covers the relevant details of the methods used such as details of the PCE, Smolyak Quadrature etc. in the appendix while not moving away from the main focus of the paper. Still, the paper could be further strengthened by clearing up a few things as described below.

1. The authors try to motivate the subject of variational analysis by mentioning that the design at nominal parameters does not guarantee that it will work under severe conditions. This is, however, quite well known and for these reasons no one designs at nominal corners and end up putting up a big enough guard band to design the system considering severe conditions. Thus, they end up being quite conservative and over-designing their system. This is where the variational analysis comes into picture to alleviate the problem of over-designing by allowing the design a better estimate of how the severe conditions impact the performance/power/temperature of the system.
\end{reviewer}
\begin{authors}
It is a very good point.
We are not able to fit everything into the current manuscript, but we will take it into consideration in the future.
Moreover, it helps us already now to respond to one of the comments of another reviewer.
\end{authors}

\begin{reviewer}
2. Section VI: The reviewer likes the use of the correlation function that captures both the correlations between the devices close to each other and also equidistant from the center of the die.
\end{reviewer}
\begin{authors}
Thank you.
\end{authors}

\begin{reviewer}
3. Section VI-A:  Some discussion on how the Nataf transformation compares with Independent component analysis which works even on non-Gaussian random variables. Some discussion on when Nataf transformation is valid or when does its accuracy suffers and what are the properties of the distributions that can be well transformed using this transformation will be useful to the reader.
\end{reviewer}
\begin{authors}
Yes, this information can certainly be helpful for the reader.
Since probability transformations were not the focal point of the paper, we tried to avoid too involved discussions and to give the reader a big picture instead.
As any other method, the Nataf transformation does have some limitations.
It is worth noting right away that this method requires only the knowledge of a covariance matrix and a set of marginal probability distributions, which are already not sufficient to fully specify a joint probability distribution.
In other words, it is yet another approximation that allows one to tackle real-world problems wherein the joint distribution is nearly never available, and this approximation was found to perform very well.
\end{authors}

\begin{reviewer}
4. Leakage Analysis: Leakage is an exponential function of gate length, this kind of shows up in the experimental results section where the authors see that the errors in the PC expansion reduce with the expansion order for 4-5 or more. The following previous approach for leakage analysis considers the leakage as exponential function of gate length and approximates the leakage as an exponential function of the PC expansion in gate length. Such exponential model will be more accurate at lower orders of expansion and still afford similar efficiency of computation as the normal PC expansion. It will be good if the authors can compare and contrast with the exponential modeling as described in:

S. Bhardwaj et.al. "A Unified approach for Full chip statistical timing and leakage analysis of nanoscale circuits considering intra-die process variations".
\end{reviewer}
\begin{authors}
Thank you for the reference.
We have included it in the paper.
Such a comparison is certainly possible; however, we were guided by different incentives, which we would like to explain now.
Even though the proposed framework covers both power and temperature, we started this work aiming to the quantification of temperature, not power as it had already been addressed by other researchers.
A PC expansion for power is a byproduct on the way to the corresponding PC expansion for temperature.
Therefore, we do not put much stress on power in the experimental results; it would make little contribution.
You are right that, if we were to focus solely on the variability of the effective channel length, it would make sense to consider some ad-hoc solutions, like the exponentiation of a PC expansion.
However, in this case, it would be more problematic to reach temperature in the end.
Moreover, we would need to find such an ad-hoc solution for every parameter we wish to account for individually, which is not the case with the current formulation as the uncertainty is injected into our framework as a ``black box''.
Thus, our approach can be applied in various situations in a straightforward manner.
\end{authors}

\begin{reviewer}
Experimental Results:

5. It is somewhat unclear how much variation in the parameters was assumed. The authors mention that the channel length was assumed to deviate by 5\%. So, does it mean that the standard deviation was 5\% or 3-sigma was 5\%. If it is the latter (3-sigma = 5\%), then the variation assumption in gate length is somewhat small. We need to see how the accuracy of the approach at some higher degrees of variation. So, some comparison with 10-15\% 3-sigma variation will be important.
\end{reviewer}
\begin{authors}
We apologize for the confusion. By ``assumed to deviate by 5\%'', we meant ``assumed to have a standard deviation of 5\%''. So, 5\% is $\sigma$, not $3 \sigma$.
We have fixed this formulation in the paper.
\end{authors}

\begin{reviewer}
6. Although PC expansion is a really good tool to estimate the system response in the presence of process variations, the authors only consider a single source of variations, i.e. gate length, and hence the error estimates and the runtimes shown will be somewhat optimistic compared to the scenario where there are more sources of variations for example: threshold voltage, oxide thickness etc. For this reason, the reviewer suggests highlighting some of the discussion related to the curse of dimensionality in the main section in addition to discussing it in the Appendix.
\end{reviewer}
\begin{authors}
Yes, the reviewer is right that the more uncertain parameters are taken into account, the more independent variables will eventually be needed for the construction of a sufficiently accurate PC expansion.
And this, of course, will intensify the curse of dimensionality.
To alleviate this issue, we perform model order reduction and thoroughly choose integration techniques.
As suggested by the reviewer, we have outlined this problem in the main section of the manuscript as well.
Regarding the experimental results, we tried to give a clear illustration of the framework and decided to focus on one parameter.
The choice of the effective channel length was a no-brainer as many studies show that it exhibits the largest deviation due to process variation and, therefore, constitutes the major concern.
\end{authors}

\begin{reviewer}
7. For practical purposes, it would be nice to see how the convergence rates and the computational speeds change with additional sources of variations such as the threshold voltage. Some commentary on this will help readers better understand the applicability of this approach.
\end{reviewer}
\begin{authors}
Yes, it is among our plans for the future research.
Speaking about the threshold voltage, as it is mentioned in the manuscript, it has an exponential dependency on the effective channel length (see [12]), and, by considering the variability of the latter, we implicitly take into consideration the variability of the former as well; although, there might be other, less severe, sources of uncertainty of the threshold voltage.
\end{authors}
