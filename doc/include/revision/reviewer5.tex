\begin{reviewer}
The paper presented statistical thermal analysis for VLSI systems using the orthogonal polynomial  (or polynomial chaos, PC) based approaches. The idea is quite straight forward as the PCs was applied to represent the transient temperatures based on the PC representations of the power inputs for given thermal models. Some standard techniques such as KL and PCA for random variable reductions and orthonormalization was also used.

However, this paper has several major issues.

\vspace{0.5em}
\noindent[Comment 1] First is the novelty. Most of the techniques (PC, PCA etc.) have been applied to many analysis and modeling problems in the CAD areas as reviewed by the authors.
\end{reviewer}
\begin{authors}
We agree that these techniques are known in our research area.
However, they are tools, and tools are supposed to be used to solve certain problems.
For example, PC expansions, as we know them now-a-days, were pioneered to address stochastic differential equations in the first place, and it was in 1991 for the classical PC (``Stochastic Finite Elements: A Spectral Approach'' by R. Ghanem \etal) and in 2002 for the generalized PC (``The Wiener-Askey Polynomial Chaos for Stochastic Differential Equations'' by D. Xiu \etal).
Since then, these tools have been applied to a wide range of uncertainty quantification problems in scientific computing.
Therefore, in our opinion, the use of such techniques in the proposed framework should not affect our contribution.
\end{authors}

\begin{reviewer}
[Comment 2] The thermal analysis is not foundamentally different than the power grid analysis.  If we just look at the eqn(2), this statistical analysis problem is even a simplified version of power grid analysis problem where the A matrix is the function of have variational parameters.
\end{reviewer}
\begin{authors}
We understand the reviewer's concern; however, we do believe that the contribution of our work is sound, and we would like to explain it now in more detail.

First and the foremost,
\begin{enumerate}
  \item We are \emph{the first to formulate and solve the problem} of transient temperature analysis of electronic systems under the uncertainty due to process variation.
\end{enumerate}
The two temperature-driven works and their drawbacks are discussed in Sec.~II.
More precisely, [Juan, 2012] considers only steady-state temperature analysis, which is considerably simpler than transient, and, moreover, is heavily based on Gaussian distributions.
[Juan, 2011] also works only under the steady-state condition and, moreover, addresses only the maximal temperature.

Let us please now directly respond to the reviewer's comment that the temperature analysis in our work is not fundamentally different from power-grid analysis studied in [Ghanta, 2006].
In the revised version of the manuscript, we have included another paper wherein the response of interconnect networks is also analyzed, namely, [Vrudhula, 2006].
Therefore, we shall discuss both.

Our first argument is general; however, we believe that it is conceptually the most important one.
From the perspective of physics, the two physical phenomena, \ie, heat conduction and electrical conduction, are fundamentally different.
Hence, conclusions with respect to power are not directly applicable to temperature as there is no a straightforward relation between them.
Consequently, the results presented in [Vrudhula, 2006] and [Ghanta, 2006] are of little help for the designer who is interested in temperature.
In contract, we provide a readily applicable framework, which guides the designer through all the steps needed to quantify temperature.

Our second argument is general as well; yet it is of the second importance for the present discussion.
The reviewer mentioned that both phenomena are modeled via systems of first-order ordinary differential equations (ODEs), which look especially similar in their state-space forms; see Eq.~(2).
However, differential equations are typical models for dynamic systems.
In numerical analysis, the solution process usually consists of a proper discretization of the state space, leading to a system of first-order ODEs.
Hence, it is natural that different problems share similar models.
The essence of the phenomena behind these models stays unchanged, which brings us to our first argument that heat transfer requires a separate treatment.

Our third and the last argument goes into the details of our particular problem and enumerates the unique aspects of our research presented in the manuscript that further differentiate us from [Vrudhula, 2006] and [Ghanta, 2006].
\begin{enumerate}
  \setcounter{enumi}{1}
  \item The perform stochastic temperature analysis takes into consideration \emph{the interdependence between leakage and temperature}.
\end{enumerate}
This aspect means that the input vector of power in Eq.~(2) is a nonlinear function of the output vector of temperature.
Hence, the system of ODEs considered in our work is \emph{nonlinear}.
Meanwhile, the systems of ODEs in [Vrudhula, 2006] and [Ghanta, 2006] are linear.
The presence of a feedback loop makes the problem more challenging and requires a different solution strategy.
It can be seen in Sec.~VI and App.~A that our solution process drastically differs from the solution processes described in [Vrudhula, 2006] and [Ghanta, 2006].

It is worth emphasizing that our stochastic power analysis \perse\ is temperature aware, which is implied by the leakage-awareness of our stochastic temperature analysis.
In contrast, all other prior works based on spectral methods for uncertainty quantification, \ie, [Vrudhula, 2006], [Ghanta, 2006], [Bhardwaj, 2006], [Bhardwaj, 2008] (another new citation included in the manuscript), and [Shen, 2009], ignore the leakage-temperature interdependence.
This interdependence, however, is of a tremendous importance, which is well known and requires no introduction; see, \eg, [Liu, 2007] and [Srivastava, 2010].

Let us return back to [Vrudhula, 2006] and [Ghanta, 2006] and continue contrasting them with the proposed framework.
The developments given in both papers are tailored for particular sets of uncertain parameters.
They rely on specific expressions, describing how the uncertain parameters influence the system, and these expression are tightly entangled with the overall solution process.
As noted by the authors of [Vrudhula, 2006], their analysis of interconnects is not limited to any particular formula modeling the influence of the internal independent variables, which is true.
However, a potential extension is not straightforward; the same applies to [Ghanta, 2006].
The reason for this is the use of so-called intrusive Galerkin projections for the computation of the coefficient of PC expansions.
This means that the user of [Vrudhula, 2006] and [Ghanta, 2006] has to adapt their solutions to every problem individually.
In contrast,
\begin{enumerate}
  \setcounter{enumi}{2}
  \item The proposed framework is \emph{flexible and can be readily applied to arbitrary parameters}.
\end{enumerate}
As mentioned in Sec.~III and further elaborated on in Sec.~V-B, our approach keeps the uncertain parameters inside a ``black box,'' which can be replaced with another box at no afford.
This type of projections is called non-intrusive [Eldred, 2008].
The user of our technique, of course, should provide such a box; however, he/she has much more freedom to do so.

In particular, the user does not have to come up with any expressions describing the joint effects of the uncertain parameters and to perform curve fitting procedures to the data from SPICE simulations (see, \eg, [Bhardwaj, 2008]) in order to determine the coefficients of the chosen expressions.
From our experience, it is rather difficult to construct a good fit for the purpose of stochastic analysis since stochastic analysis tries to explore as large a portion of the probability space as possible, and, thus, this fit should capture very well broad ranges of the parameters.
In this regard, our preliminary research shown that the fits with polynomials of low orders (pure polynomials and rational functions w/o exponentiation) can introduce significant errors, which is also noted in [Shen, 2009].
As mentioned in Sec.~VI-B, in our experiments, we utilize linear interpolation without any polynomial-based fitting, which our framework readily allows us to do.

Next, the methodology described in Sec.~V in [Vrudhula, 2006] does not address the case when the uncertain parameters are dependent; a potential solution is mentioned only in the context of Gaussian distributions.
On the other hand, [Ghanta, 2006] is founded on the basis of the \emph{continuous} version of the KL decomposition and, therefore, assumes that the correlation function of the uncertain parameters is known (the same in [Bhardwaj, 2006] and [Bhardwaj, 2008]).
Such an assumption, however, is impractical, which is also noted by [Vrudhula, 2006].
In reality, it can be difficult to make a justifiable choice and tune such a correlation function.
In contrast,
\begin{enumerate}
  \setcounter{enumi}{3}
  \item The proposed framework \emph{systematically addresses dependencies} and has mild requirements on the prior knowledge of these dependencies.
\end{enumerate}
As described in Sec.~V-A and Sec.~VI-A, the knowledge of a correlation matrix is sufficient to characterize dependencies for our approach.
Such a matrix can readily be estimated from measurements and, thus, is a more probable input to stochastic analyses.
Note that we also assume a correlation function, Eq.~(13).
However, although it is rather unique and captures well certain features inherent to the fabrication process, we use this function with the only purpose of constructing correlation matrices for our experiments due to the above-mentioned reasons.

Further, [Vrudhula, 2006] states that the proposed methodology can be applied to arbitrary distributions, which is true.
However, the only illustrated case is Gaussian.\footnote{The authors also discuss log-normal parameters; however, this case is reduced to Gaussian due to the natural relation between the two families of distributions.}
In a non-Gaussian setting with dependencies, PC expansions are not straightforward.
The reason is that mutual independence is essential for PC expansions, and linear transformations, removing correlations, in general, do not yield independence.
On the other hand, [Ghanta, 2006] is tightened to Gaussian distributions, which are critical for the development in that paper (the same in [Bhardwaj, 2006] and [Bhardwaj, 2008]).
In contrast,
\begin{enumerate}
  \setcounter{enumi}{4}
  \item The proposed framework \emph{works with arbitrary distributions}\footnote{The applicability to arbitrary distributions is discussed later in our response to the corresponding comment of the current reviewer.} and is illustrated in a general setting using the Nataf transformation.
\end{enumerate}
Dependent non-Gaussian variables are addressed via the Nataf transformation, which has not received much attention in our research area.
Our paper demonstrates this powerful technique.
At this point, we also would like to note that, in the manuscript,
\begin{enumerate}
  \setcounter{enumi}{5}
  \item Physically-bounded variations are modeled using beta distributions.
\end{enumerate}
We have not encountered such a model in the literature; however, it has its own right due to the boundedness of the support, as we explain in Sec.~VII.

Taking into consideration the argumentation given above, we conclude that our research makes a prominent contribution.
The manuscript provides a systematic description of all the stages involved in the uncertainty quantification process of transient temperature profiles.

\done{The prior works have been described in more detail in Sec.~II.}

\done{The nonlinearity of the thermal system has been emphasized in Sec.~V-C.}

\done{The benefits of non-intrusive PC expansions have been emphasized at the end of Sec.~V-D.}

\done{The benefits of working with correlation matrices instead of correlation functions has been explained in Sec.~VI.}
\end{authors}

\begin{reviewer}
[Comment 3] Second is the motivation for the statistical thermal analysis problem. Many existing works mainly focus on the power or power grids as we can determine the chip yields once the powers or timing (due to power grids) distributions are known. If we limit the power to the given ranges, the temperature variations will be limited to a safe ranges. As a result, verify the temperature variations seems less important than verifying the power variations. Also for multi-core and many-core architectures, the temperatures are highly loads dependent and the variations due to process variations maybe still secondary effects for a chip designers. Temperature is also environment and boundary conditions (fan speeds, liquid speed) dependent
\end{reviewer}
\begin{authors}
Let us please explain our motivation in more detail.

First, we believe that power analysis and temperature analysis are inseparable.
The reason is the ever so important interdependence between temperature and leakage: high temperatures skyrocket the leakage power, and the leakage power strikes back by cranking up the dissipation of heat.
Process variation, which has been rapidly increasing over the last decade due to technology scaling, magnifies the aforementioned concern even further.
Therefore, stochastic power analysis has to be accompanied by stochastic temperature analysis.
The proposed framework is self-contained from this perspective, which differentiates us from the prior works as discussed in our previous answer.

Second, the immediate cause of various reliability issues (\eg, time-dependent dielectric breakdown, electro-migration, and thermal-cycling fatigue) is temperature, not power \perse.
Moreover, for certain failure mechanisms, the failure rate has an exponential dependency on the operating temperature [Juan, 2012].
In order to alleviate such issues, the designer indeed can try to constrain power and hope that temperature will get squeezed into safe ranges.
In this regard, we would like to respond with the words of Reviewer 3.
Such strategies ``[\ldots] can end up putting up a big enough guard band to design the system considering severe conditions.
Thus, they end up being quite conservative and over-designing their system.''
In other words, the more we know about the quantities we care about and the more immediate/direct this knowledge is, the more efficient and effective control we have over those important quantities.
Therefore, if we are to repress temperature, it is wiser to study this very temperature instead of power.
In addition, since power does not translate to temperature in a straightforward manner, the well-established reliability models based on temperature will be excluded from the arsenal of the designer when only stochastic power analysis is at the designer's disposal.
This implies, for example, that various temperature-dependent design-space-exploration procedures will not be possible to undertake.

We agree with the reviewer that temperature is load dependent (which applies to power equally well) and can be influenced by other conditions.
The focal point of this paper is process variation, and the aforementioned factors are left for the future work.
However, it is worth noting that the effect of process variation on temperature is substantial, in particular, due the leakage-temperature interdependence discussed earlier.
This is also emphasized in other temperature-related studies.
For instance, [Juan, 2011] gives an example wherein the maximal temperature fluctuates within a band of around $20^\circ$C, depending on the severity of process variation.
Our example in Fig.~1 indicates a similar sensitivity of temperature to process variation.

To conclude, the stochastic analysis presented in the manuscript is well motivated.

\done{Our motivation has been explained in more detail in Sec.~I.}

\done{It has been noted that process variation is not the only source of uncertainty in Sec.~IV.}
\end{authors}

\begin{reviewer}
[Comment 4] In addition, the paper only consider the effective channel length, which makes the paper weak as different parameters may exhibits different correlations patterns (strong or weak). In case of weak correlations, the standard KL and PCA method will not work well as they canâ€™t reduce many variables in this case. This may be the case even for the effective channel length. The authors only consider exponential correlation models.
\end{reviewer}
\begin{authors}
We would like to note first that the proposed framework does not pose any constraints on the considered uncertain parameters $\vU(\o)$ including their correlations.

The reviewer is absolutely right that different correlation patterns make the model order reduction procedure work differently.
As a result, two sets of initial uncertain parameters $\vU'(\o)$ and $\vU''(\o)$ of the same cardinality but with different correlation structures can end up in two sets of independent random variables $\vZ'(\o)$ and $\vZ''(\o)$ whose cardinalities arbitrary differ from each other.
Therefore, it is difficult to make a connection between the dimensionality of $\vU(\o)$ with the dimensionality of $\vZ(\o)$.
At the same time, the applicability of our approach, which is the main concern of the comment, is essentially dictated by the dimensionality of $\vZ(\o)$.
Therefore, $\vZ(\o)$ is a better indicator of this applicability than $\vU(\o)$, and the latter can be even misleading.

At this point, we kindly ask the reviewer to consider our response to the seventh comment of Reviewer 3 wherein we discuss the above issue in more detail and give an illustrative example.
In particular, we justify there the fact that the results in Tab.~V, which include the dimensionality of $\vZ(\o)$, address well the aforementioned applicability concern.

Let us continue our discussion here and emphasize that the effect of correlation patterns is indeed important; we are in a perfect alignment with the reviewer in this regard.
For this particular reason, the experimental results are given for various values of the $\eta$ parameter, which controls these patterns and, thus, illustrates their influence.
However, the goal was to illustrate and explain rather than to give a broad overview of possible correlation structures.
Therefore, we decided to take the Ornstein-Uhlenbeck kernel, which is used in many papers (see, \eg, [Ghanta 2006], [Bhardwaj, 2006], [Bhardwaj, 2008], and [Shen, 2009]) and is used alone, and combine it with the squared-exponential kernel, which has not received much attention in the literature.
In some cases, the lack of attention to the second kernel can be explained by the lack of an analytical solution for this correlation function with respect to the continuous KL decomposition (utilized in [Ghanta 2006], [Bhardwaj, 2006], and [Bhardwaj, 2008]).
From this perspective, our exploration of correlation patterns is more comprehensive than the ones in the prior works.

Let us please also comment separately on the choice of the effective channel length.
As with correlations, the goal was to give a clear illustration, and, therefore, we decided to pick one parameter.
Our choice was guided by the reasoning given in [Juan, 2011, 2012].
To elaborate, the two main components of the total leakage current are the subthreshold and gate leakage currents.
The former constitutes the major concern since the importance of the latter has been recently reduced with the introduction of high-k dielectrics.
Then, apart from temperature, the subthreshold leakage current has a strong dependency on the effective channel length and threshold voltage, which are severely deteriorated by process variation.
The threshold voltage is most sensitive to the effective channel length and gate oxide thickness with the latter being the weakest one [Shen, 2010].
Thus, the effective channel length was chosen.
Consequently, the proposed framework is exemplified on arguably the most crucial process parameter.

\done{The applicability of our approach has been discussed in connection with Tab.~V in Sec.~VII-B, which also addresses the seventh comment of Reviewer 3.}
\end{authors}

\begin{reviewer}
[Comment 5] The claim that the proposed method can be applied to arbitrary distributions are not correct. As the author mentioned, that the resulting temperature or even the power distributions are not known a priori. As a result, we do not know which orthogonal polynomials we should use. This actually is the major limitations of the PC based method.
\end{reviewer}
\begin{authors}
First of all, we would like to clarify the formulation used in the manuscript.
In the abstract, we write ``diverse probability laws and arbitrary impacts of the underlying uncertain parameters.''
So, the word \emph{arbitrary} belongs to \emph{impacts} and refers to the fact that the influence of the uncertain parameters is introduced into the framework as a ``black box,'' as it is unfolded in Sec.~III.
The formulation was ambiguous and has been modified in the revised version of the manuscript.

Despite the fact that we do not use the word \emph{arbitrary}, the theory of PC expansions can indeed be applied to arbitrary distributions (which is also emphasized in other papers such as [Vrudhula, 2006]), and we would like to justify it now.

Consider a stochastic system.
The output of the system is a stochastic process.
By the Cameron-Martin theorem established in 1947, the space of square-integrable (finite variance) stochastic processes is a separable Hilbert space wherein the countable orthonormal basis is formed by the Hermite polynomials.
Therefore, arbitrary square-integrable stochastic process admits an expansion, convergent in the mean-square sense, with respect to the basis.
In 2002, this result was generalized to the hypergeometric orthogonal polynomials from the Askey scheme in ``The Wiener-Askey Polynomial Chaos for Stochastic Differential Equations'' by D. Xiu \etal

Let us now look at the system from a more practical perspective.
The system is stochastic due to its internal dependence on a set of uncertain parameters $\vU(\o)$.
As the reviewer correctly noted, the output distribution is not known \apriori.
Therefore, whenever it comes to the construction of a PC expansion, one is typically guided by other factors in order to choose an adequate polynomial basis.
One such factor is the distributions of the independent random variables $\vZ(\o)$, which are either $\vU(\o)$ in the absence of dependencies or the result of the preprocessing stage in the presence of dependencies.
Then, many standard distributions directly correspond to certain families of orthogonal polynomials from the Askey scheme.
When the reviewer says that PC expansions are not applicable to arbitrary distributions, he/she probably refers to the fact that not all probability distributions have such a direct correspondence with orthogonal polynomials, which is true.
However, one can always re-parametrize the problem by transforming one probability distribution into another, for example, to the one that does have a suitable polynomial basis.
Please refer to the above paper by D. Xiu \etal, more precisely, to Sec.~6 titled ``Representation of Arbitrary Random Inputs.''
In addition, one does not have to perform such a re-parametrization and can construct custom orthogonal polynomials instead.
Please refer to ``Modeling Arbitrary Uncertainties Using Gram-Schmidt Polynomial Chaos'' by J. Witteveen \etal

We agree with the reviewer in the sense that the \emph{optimal} polynomial basis can indeed be unknown, and the expansion order can be rather high to reach the desired level of accuracy.
Nevertheless, PC expansions are still legitimate and converge in the mean-square sense.

Let us now explain why we prefer to use the word \emph{diverse} instead of \emph{arbitrary}.
Such ambitious statements as ``applicable to arbitrary distributions'' nearly always some formal assumptions underneath, which can formally invalidate these statements, even though the assumptions fail only for a few pathological cases.
For PC expansions, it is the finite-variance assumption mentioned above and in Sec.~V-B of the manuscript.
However, most physical processes have finite variance; see the above paper by D. Xiu \etal{}
If the reviewer refers to this assumption, we apologize for our misunderstanding and remind that \emph{arbitrary} is not present in the manuscript.

\done{The formulation in the abstract has been clarified.}

\done{Possible solutions of general distributions have been outlined in App.~C.}
\end{authors}

\begin{reviewer}
[Comment 6] The paper did not address one important problems in thermal analysis: how the leakage and temperature interplay and its impacts on the statistical thermal analysis?
\end{reviewer}
\begin{authors}
The stochastic power-temperature analysis presented in the paper does take into consideration the interdependence between leakage and temperature.
The general dependency of power on temperature can be seen in Eq.~(1) and is emphasized at the end of Sec.~V-D3.
The leakage-temperature interdependence is mentioned at the end of Sec.~V-B and in Sec.~VI-B.
Therefore, all the experimental results explore this important aspect.

However, we indeed did not write anything about this interdependence in earlier sections; it has been fixed in the revised version of the manuscript.

\done{The importance of the leakage-temperature interdependence has been mentioned in Sec.~I.}

\done{The negligence of the interdependence by the prior works has been emphasized in Sec.~II.}

\done{The fact that our framework accounts for the interdependence has been noted in Sec.~III.}
\end{authors}

\begin{reviewer}
[Comment 7] The paper has many math notations. It will be nice to have table of all the math notations at the beginning of  the paper.
\end{reviewer}
\begin{authors}
We agree with the reviewer.
Such a table has been introduced on page 2.
In order to make room for the table, we remove Tab.~1---it was illustrating the correspondence between probability distributions and polynomial bases---since this information can be easily found in the cited literature.
Also, due to the same shortage of space, we had to constrain the list of notations and, thus, cherry-picked only the most relevant notations from our perspective.

\done{A table with the main notations has been introduced on page 2.}
\end{authors}

\begin{reviewer}
In the experimental section:

\vspace{0.5em}
\noindent[Comment 8] The number of MC samples (10\^{}4) seems too big. For many practical problems, MC method with a few hundreds or thousands of samples will be good enough to get a good distributions.
\end{reviewer}
\begin{authors}
The choice of $\nsamples = 10^4$ for the comparison of the computational speed has three major aspects.

The first aspect is the experience from other studies.
For example, [Ghanta, 2006] analyzes power grids and uses $5 \times 10^3$ samples; [Shen, 2009] models the full-chip leakage power and uses $5 \times 10^4$ samples; [Chandra, 2010] performs system-level power analysis and uses $10^4$ samples; and [Cheng, 2011] studies across-wafer variations and uses $10^5$ samples.
Thus, we conclude that $\nsamples = 10^4$ is not an excess in our research area.
One should also take into consideration the fact that heat transfer is one level above power, and, therefore, the complexity of temperature analysis can reasonably be higher than the one of power analysis.
For this reason, the number of MC samples can also increase to retain the same accuracy.

The second aspect is the theoretical results given in [I. D\'{i}az-Emparanza, 2002].
For example (taken from an earlier publication of I. D\'{i}az-Emparanza), in order to approach the 0.05-probability tail of a probability distribution with confidence 99\% and accuracy 0.005, the estimated number of samples is roughly 12550.
This result, of course, is very general and applicable in any context.
However, it tells us that the chosen value for $\nsamples$ makes sense.

The third aspect, which is the most important one, is our own experience and observations from the tables thoroughly discussed in Sec.~VII-A.
Moreover, as we write in the paper, we do believe that $\nsamples = 10^4$ is a lower bound.
However, we tried to avoid any bias towards the proposed technique, which could happen if we raised $\nsamples$ to $10^5$.

Let us also note the following.
All the prior studies listed in Sec.~II that perform comparisons with MC simulations in terms of accuracy and/or speed postulate their choices regarding the number of MC samples.
In contrast, we explain our choice and provide evidence to support it.

\done{The discussion about the choice of $\nsamples = 10^4$ has been reformulated, moved from Sec.~VII-A to Sec.~VII-B, and supported with additional citations.}
\end{authors}

\begin{reviewer}
[Comment 9] Many pesudo MC methods actually have much faster convergent rate with much less number of samples actually.
\end{reviewer}
\begin{authors}
We agree with the reviewer that there are other sampling techniques, such as Latin hypercube and quasi-MC sampling, which try to improve upon the convergence properties of the classical MC sampling.
However, their applicability is often limited due to additional restrictions posed by the design of these methods [Xiu, 2010].
For example, quasi-MC sampling can indeed be faster than pure MC sampling since the convergence rates are $\O_1 = \O((\log\nsamples)^\nvars / \nsamples)$ and $\O_2 = \O(1 / \sqrt{\nsamples})$ for the former and latter, respectively.
However, $\O_1$ is smaller than $\O_2$ only when $\nvars$ is small and $\nsamples$ is large.
In particular, already with $\nvars = 4$, the curve corresponding to $\O_1$ is higher than the one of $\O_2$ for $\nsamples$ from around 50 to $3 \times 10^6$.
In our experimental results, $\nvars$ goes up to 12.

Let us please also note that MC sampling was not the direction we wanted to put our efforts into: our major focus is at PC expansions.

\done{Alternative sampling methods have been mentioned in Sec.~II.}
\end{authors}

\begin{reviewer}
[Comment 10] The table V and table VI look  bit weird.  It seems strange that the PC method just takes a few seconds, while MC method will takes hours. The speedups in the two tables  are even bigger than number of MC samples (10\^{}4) in many cases. This means that one PC-based computing take less than one MC sample run, which is impossible.  More explanations are needed.
\end{reviewer}
\begin{authors}
Thank you for the excellent observation.
Let us explain how this speed has been achieved.

First of all, recall that we utilize quadrature-based non-intrusive projections for the calculation of the coefficients of PC expansions.
The reviewer is right that, if we were to construct such non-intrusive PC expansions in the traditional way, as it is usually presented in the corresponding literature (see, \eg, [Eldred, 2008]), the proposed framework could not be faster that one MC simulation.
To be precise, the speedup would be exactly the ratio between the number of MC samples $\nsamples$ and the number of quadrature nodes $\qdorder$ utilized for non-intrusive projections inside our approach; we would not need to perform any actual time measurements.

To elaborate, the beauty of non-intrusive projections is in the fact that well-established deterministic codes require no modifications in order to be analyzed from the stochastic perspective when the input to these codes becomes stochastic.
Therefore, when one has a deterministic solver for some problem and wishes to quantify this problem under a stochastic input, he/she takes this solver \emph{as is} and plugs it in into an appropriate uncertainty quantification framework, which calls this code a number of times with different (deterministic, fixed) inputs according to a certain strategy.
In our case, this strategy is the chosen quadrature rule.
Therefore, as many nodes are in the rule, that many times the solver will be called.
Consequently, if we followed the procedure described above, our approach would be racing with the MC-based approach only in terms of the number of calls to a numerical solver of the system of differential equations given in Eq.~(2) (in its deterministic form, \ie, when $\o$ or, equivalently, $\vU(\o)$ is fixed).

However, our solution diverges from the traditional construction of non-intrusive PC expansions.
We go inside the solver and perform PC expansions in a step-wise manner following the recurrence given in Eq.~(3), which eventually leads to the recurrence for the PC coefficients in Eq.~(7).
The advantages of such an approach are summarized in the last two paragraphs of Sec.~VI-D (the very last one has been added in the revised version of the manuscript).
As described in Sec.~VI-C and App.~A, the derivation of this organic part of out framework is done under the assumption that the granularity of power/temperature profiles is sufficiently small such that the power consumption stays approximately constant within one time interval.

On the other hand, the MC-based approach is a brute force.
The accuracy of this approach is not compromised by any additional assumptions: there is no model order reduction inside and the thermal system is solved directly using traditional numerical techniques; in our case, it is Runge-Kutta formulae, as described at the beginning of Sec.~VII.

Consequently, the reported speedups of the proposed framework are due to two major factors: PC expansions \perse\ and the recurrence-based solution process of the thermal model.

For a better intuition of the computational speed of the proposed framework, we kindly ask the reviewer to consider our answer to the first comment of Reviewer 4 wherein an analysis of the time complexity is given.

\done{The two contributors to the speedup of the proposed framework have been highlighted in Sec.~VII-B.}
\end{authors}
