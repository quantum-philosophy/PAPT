\begin{reviewer}
The paper presented statistical thermal analysis for VLSI systems using the orthogonal polynomial  (or polynomial chaos, PC) based approaches. The idea is quite straight forward as the PCs was applied to represent the transient temperatures based on the PC representations of the power inputs for given thermal models. Some standard techniques such as KL and PCA for random variable reductions and orthonormalization was also used.

However, this paper has several major issues.
\end{reviewer}
\begin{authors}
Thank you for your constructive critique given in what follows.
\end{authors}

\begin{reviewer}
First is the novelty. Most of the techniques (PC, PCA etc.) have been applied to many analysis and modeling problems in the CAD areas as reviewed by the authors.
\end{reviewer}
\begin{authors}
We agree that these techniques are known in our research area.
However, they are just tools, and tools are supposed to be used to solve certain problems.
For example, PC expansions, as we know them now-a-days, were pioneered to address stochastic differential equations in the first place, and it was in 1991 for the classical PC (``Stochastic Finite Elements: A Spectral Approach'' by R. Ghanem \etal) and in 2002 for the generalized PC (``The Wiener-Askey Polynomial Chaos for Stochastic Differential Equations'' by D. Xiu \etal).
Since then, these tools have been applied to a wide range of uncertainty quantification problems in scientific computing.
Therefore, the fact that the aforementioned techniques are a part of our framework does not affect our contribution.
\end{authors}

\begin{reviewer}
The thermal analysis is not foundamentally different than the power grid analysis.  If we just look at the eqn(2), this statistical analysis problem is even a simplified version of power grid analysis problem where the A matrix is the function of have variational parameters.
\end{reviewer}
\begin{authors}
We understand the reviewer's concern; however, we do believe that the contribution of our work is sound, and we would like to explain it now in more detail.

First and the foremost,
\begin{enumerate}
  \item We are \emph{the first to formulate and solve the problem} of transient temperature analysis of electronic systems under the uncertainty due to process variation.
\end{enumerate}
The two temperature-driven works and their drawbacks are discussed in Sec.~II.
More precisely, [Juan, 2012] considers only steady-state temperature analysis, which is considerably simpler than transient, and, moreover, is heavily based on Gaussian distributions.
[Juan, 2011] also works only under the steady-state condition and, moreover, addresses only the maximal temperature.

Let us now directly respond to the reviewer's claim that the temperature analysis in our work is no fundamentally different from power-grid analysis studied in [Ghanta, 2006].
In the revised version of the manuscript, we have included another paper wherein the response of interconnect networks is also analyzed, namely, [Vrudhula, 2006].
Therefore, we shall discuss both.

Our first argument is general; however, in our opinion, it is conceptually the most important one.
From the perspective of physics, the two physical phenomena, \ie, heat conduction and electrical conduction, are fundamentally different.
Temperature is one level above power, and, therefore, temperature analysis embraces the complexity associated with power analysis and justifiably increases it.
Therefore, conclusions with respect to power are not directly applicable to temperature.
Consequently, the results presented in [Vrudhula, 2006] and [Ghanta, 2006] are of little help for the designer who is interested in temperature.
In contract, we provide a readily applicable framework, which guides the designer through all the steps needed to quantify temperature.

Our second argument is general as well; yet it is of the second importance for the present discussion.
The reviewer justifies the aforementioned claim by the fact that both phenomena are modeled via systems of first-order ordinary differential equations (ODEs), which look especially alike in their state-space forms; see Eq.~(2).
However, differential equations are typical models for dynamic systems.
In numerical analysis, the solution process usually consists of a proper discretization of the state space, leading to a system of first-order ODEs.
Hence, it is natural that different problems share similar models.
The essence of the phenomena behind these models stays unchanged, which brings us to our first argument: heat transfer requires a separate treatment.

Our third and the last argument goes into the details of our particular problem and enumerates the unique aspects of our manuscript, which further differentiates us from [Vrudhula, 2006] and [Ghanta, 2006].
\begin{enumerate}
  \setcounter{enumi}{1}
  \item The perform stochastic temperature analysis takes into consideration \emph{the interdependence between leakage and temperature}.
\end{enumerate}
This aspect means that the input vector of power in Eq.~(2) is a nonlinear function of the output vector of temperature.
Hence, the system of ODEs considered in our work is \emph{nonlinear}.
Meanwhile, the systems of ODEs in [Vrudhula, 2006] and [Ghanta, 2006] are linear.
The presence of a feedback loop makes the problem more challenging and requires a different solution strategy.
It can be seen in Sec.~VI and App.~A that our solution process drastically differs from the solution processes described in [Vrudhula, 2006] and [Ghanta, 2006].

It is worth emphasizing that our stochastic power analysis \perse\ is temperature aware, which is implied by the leakage-awareness of our stochastic temperature analysis.
In contrast, all other prior works concerned with process variation at the device level and based on spectral methods for uncertainty quantification, \ie, [Ghanta, 2006], [Bhardwaj, 2006], [Bhardwaj, 2008] (another new citation included in the manuscript), and [Shen, 2009], ignore the leakage-temperature interdependence.
This interdependence, however, is of a tremendous importance, which is well known and requires no introduction; see, \eg, [Liu, 2007] and [Srivastava, 2010].

Let us return back to [Vrudhula, 2006] and [Ghanta, 2006] and continue contrasting them with the proposed framework.
The developments given in both papers are tailored for particular sets of uncertain parameters.
They rely on specific expressions, describing how the uncertain parameters influence the system, and these expression are tightly entangled with the overall solution process.
As noted by the authors of [Vrudhula, 2006], their analysis of interconnects is not limited to any particular formula modeling the influence of the internal independent variables, which is true.
However, a potential extension is not straightforward; the same applies to [Ghanta, 2006].
The reason for this is the use of so-called intrusive Galerkin projections for the computation of the coefficient of PC expansions.
This means that the user of [Vrudhula, 2006] and [Ghanta, 2006] has to adapt their solutions to every problem individually.
In contrast,
\begin{enumerate}
  \setcounter{enumi}{2}
  \item The proposed framework is \emph{flexible and can be readily applied to arbitrary parameters}.
\end{enumerate}
As mentioned in Sec.~III and further elaborated on in Sec.~V-B, our approach keeps the uncertain parameters inside a ``black box,'' which can be replaced with another box at no afford.
This type of projections is called non-intrusive [Eldred, 2008].
The user of our technique, of course, should provide such a box; however, he/she has much more freedom to do so.

In particular, the user does not have to come up with any expressions describing the joint effects of the uncertain parameters and to perform curve fitting procedures to the data from SPICE simulations (see, \eg, [Bhardwaj, 2008]) in order to determine the coefficients of the chosen expressions.
From our experience, it is rather difficult to construct a good fit for the purpose of stochastic analysis since stochastic analysis tries to explore as large a portion of the probability space as possible, and, thus, this fit should capture very well broad ranges of the parameters.
In this regards, our preliminary research shown that the fits with polynomial of low orders (pure polynomials and rational functions w/o exponentiation) can introduce significant errors, which is also noted in [Shen, 2009].
As mentioned in Sec.~VI-B, in our experiments, we utilize linear interpolation without any polynomial-based fitting, which our framework readily allows to do.

Next, the methodology described in Sec.~V in [Vrudhula, 2006] does not address the case when the uncertain parameters are correlated; a potential solution is mentioned only in the context of Gaussian distributions.
On the other hand, [Ghanta, 2006] is founded on the basis of the \emph{continuous} version of the KL decomposition and, therefore, assumes that the correlation function of the uncertain parameters is known (the same in [Bhardwaj, 2006] and [Bhardwaj, 2008]).
Such an assumption, however, is impractical, which is also noted by [Vrudhula, 2006].
In reality, it can be difficult to make a justifiable choice and tune such a correlation function.
In contrast,
\begin{enumerate}
  \setcounter{enumi}{3}
  \item The proposed framework \emph{systematically addresses correlations} and has mild requirements on the prior knowledge of these correlations.
\end{enumerate}
As described in Sec.~V-A and Sec.~VI-A, the knowledge of a correlation matrix is sufficient for our approach.
Such a matrix can readily be estimated from measurements and, thus, is a more probable input to stochastic analyses.
Note that we also assume a correlation function, Eq.~(13).
However, although it is rather unique and captures well certain features inherent to the fabrication process, we use this function with the only purpose of constructing correlation matrices for our experiments due to the above-mentioned reasons.

Next, [Vrudhula, 2006] states that the proposed methodology can be applied to arbitrary distributions, which is true.
However, the only illustrated case is Gaussian.\footnote{The authors also discuss log-normal parameters; however, this case is reduced to Gaussian due to the natural relation between the two families of distributions.}
In a non-Gaussian setting with correlations, PC expansions are not straightforward.
The reason is that mutual independence is essential for PC expansions, and liner transformations, removing correlations, in general, do not yield independence.
On the other hand, [Ghanta, 2006] is tightened to Gaussian distributions, which are critical for the development in that paper (the same in [Bhardwaj, 2006] and [Bhardwaj, 2008]).
In contrast,
\begin{enumerate}
  \setcounter{enumi}{4}
  \item The proposed framework \emph{works with arbitrary distributions}\footnote{The applicability to arbitrary distributions is discussed later in our response to the corresponding comment of the current reviewer.} and is illustrated in a general setting using the Nataf transformation.
\end{enumerate}
Correlated non-Gaussian variables are addressed via the Nataf transformation, which has not received much attention in our research area.
Our paper demonstrates this powerful technique.
At this point, we also would like to note that, in the manuscript,
\begin{enumerate}
  \setcounter{enumi}{5}
  \item Physically-bounded variations are modeled using beta distributions.
\end{enumerate}
We have not encountered such a model in the literature; however, it has its own right due to the boundedness of the support, as we explain in Sec.~VII.

Taking into consideration the argumentation given above, we conclude that our research makes a prominent contribution.
The manuscript provides a systematic description of all the stages involved in the uncertainty quantification process of transient temperature profiles.

\done{The prior works have been described in more detail in Sec.~II.}

\done{The nonlinearity of the thermal system has been emphasized in Sec.~V-C.}

\done{The benefits of non-intrusive PC expansions have been emphasized at the end of Sec.~V-D.}

\done{The benefits of working with correlation matrices instead of correlation functions has been explained in Sec.~VI.}
\end{authors}

\begin{reviewer}
Second is the motivation for the statistical thermal analysis problem. Many existing works mainly focus on the power or power grids as we can determine the chip yields once the powers or timing (due to power grids) distributions are known. If we limit the power to the given ranges, the temperature variations will be limited to a safe ranges. As a result, verify the temperature variations seems less important than verifying the power variations. Also for multi-core and many-core architectures, the temperatures are highly loads dependent and the variations due to process variations maybe still secondary effects for a chip designers. Temperature is also environment and boundary conditions (fan speeds, liquid speed) dependent
\end{reviewer}
\begin{authors}
Let us please explain our motivation in detail.

First, we believe that power analysis and temperature analysis are inseparable.
The reason is the ever so important interdependence between temperature and leakage: high temperatures skyrocket the leakage power, and the leakage power strikes back by cranking up the dissipation of heat.
Process variation, which has been rapidly increasing over the last decade due to technology scaling, magnifies the aforementioned concern even further.
Therefore, stochastic power analysis has to be accompanied by stochastic temperature analysis.
The proposed framework is self-contained from this perspective, which differentiates us from the prior works as discussed in our previous answer.

Second, the immediate cause of various reliability issues (\eg, time-dependent dielectric breakdown, electro-migration, and thermal-cycling fatigue) is temperature, not power \perse.
Moreover, for certain failure mechanisms, the failure rate has an exponential dependency on the operating temperature [Juan, 2012].
In order to alleviate such issues, the designer indeed can try to constrain power and hope that temperature will get squeezed into safe ranges.
In this regard, we would like to respond with the words of Reviewer 3.
Such strategies ``[\ldots] can end up putting up a big enough guard band to design the system considering severe conditions.
Thus, they end up being quite conservative and over-designing their system.''
In other words, the more we know about the quantities we care about and the more immediate/direct this knowledge is, the more efficient and effective control we have over those important quantities.
Therefore, if we are to repress temperature, it is wiser to study this very temperature instead of power.
In addition, since power does not translate to temperature in a straightforward manner, the well-established reliability models based on temperature will be excluded from the arsenal of the designer when only stochastic power analysis is at the designer's disposal.
This implies, for example, that various temperature-dependent design-space-exploration procedures will not be possible to undertake.

It is true that temperature is load dependent (which applies to power equally well) and can be influenced by other conditions.
The focal point of this paper is process variation, and the aforementioned factors are left for the future work.
However, it is worth noting that the effect of process variation on temperature is substantial, in particular, due the leakage-temperature interdependence discussed earlier.
This is also emphasized in other temperature-related studies.
For instance, [Juan, 2011] gives an example wherein the maximal temperature fluctuates within a band of around $20^\circ$C, depending on the severity of process variation.
Our example in Fig.~1 indicates a similar sensitivity of temperature to process variation.

To conclude, the stochastic analysis presented in the manuscript is well motivated.

\done{Our motivation has been explained in more detail in Sec.~I.}

\done{It has been noted that process variation is not the only source of uncertainty in Sec.~IV.}
\end{authors}

\begin{reviewer}
In addition, the paper only consider the effective channel length, which makes the paper weak as different parameters may exhibits different correlations patterns (strong or weak). In case of weak correlations, the standard KL and PCA method will not work well as they canâ€™t reduce many variables in this case. This may be the case even for the effective channel length. The authors only consider exponential correlation models.
\end{reviewer}
\begin{authors}
Let us please first remind that the proposed framework do not pose any constraints on the considered uncertain parameters $\vU(\o)$ including their correlations.

The reviewer is absolutely right that different correlation patterns make the model order reduction procedure work differently.
As a result, two sets of initial uncertain parameters $\vU'(\o)$ and $\vU''(\o)$ of the same cardinality but with different correlation structures can end up in two sets of independent random variables $\vZ'(\o)$ and $\vZ''(\o)$ whose cardinalities arbitrary differ from each other.
Therefore, it is difficult to make a connection between the dimensionality of $\vU(\o)$ with the dimensionality of $\vZ(\o)$.
At the same time, the applicability of our approach, which is the main concern of the comment, is essentially dictated by the dimensionality of $\vZ(\o)$.
Therefore, $\vZ(\o)$ is a better indicator of this applicability than $\vU(\o)$, and the latter can be even misleading.

At this point, we kindly ask the reviewer to consider our response to the seventh comment of Reviewer 3 wherein we discuss the above issue in more detail and give an illustrative example.
In particular, we justify there the fact that the results in Tab.~V, which include the dimensionality of $\vZ(\o)$, address well the aforementioned applicability concern.

Let us continue our discussion here and emphasize that the effect of correlation patterns is indeed important; we are in a perfect alignment with the reviewer in this regard.
For this particular reason, the experimental results are given for various values of the $\eta$ parameter, which controls these patterns and, thus, illustrates their influence.
However, the goal was to illustrate and explain rather than to give a broad overview of possible correlation structures.
Therefore, we decided to take the Ornstein-Uhlenbeck kernel, which is used in many papers (see, \eg, [Ghanta 2006], [Bhardwaj, 2006], [Bhardwaj, 2008], and [Shen, 2009]) and is used alone, and combine it with the squared-exponential kernel, which has not received much attention in the literature.
In some cases, the lack of attention to the second kernel can be explained by the lack of an analytical solution for this correlation function with respect to the continuous KL decomposition (utilized in [Ghanta 2006], [Bhardwaj, 2006], and [Bhardwaj, 2008]).
From this perspective, our exploration of correlation patterns is more comprehensive than the one in the prior works.

Let us please also comment separately on the choice of the effective channel length.
As with correlations, the goal was to give a clear illustration, and, therefore, we decided to pick one parameter.
Our choice was guided by the reasoning given in [Juan, 2011, 2012].
To elaborate, the two main components of the total leakage current are the subthreshold and gate leakage currents.
The former constitutes the major concern since the importance of the latter has been recently reduced with the introduction of high-k dielectrics.
Then, apart from temperature, the subthreshold leakage current has a strong dependency on the effective channel length and threshold voltage, which are severely deteriorated by process variation.
The threshold voltage is most sensitive to the effective channel length and gate oxide thickness with the latter being the weakest one [Shen, 2010].
Thus, the effective channel length was chosen.
Therefore, the proposed framework is exemplified on arguably the most crucial process parameter.

\done{The applicability of our approach has been discussed in connection with Tab.~V in Sec.~VII-B, which also addresses the seventh comment of Reviewer 3.}
\end{authors}

\begin{reviewer}
The claim that the proposed method can be applied to arbitrary distributions are not correct. As the author mentioned, that the resulting temperature or even the power distributions are not known a priori. As a result, we do not know which orthogonal polynomials we should use. This actually is the major limitations of the PC based method.
\end{reviewer}
\begin{authors}
Let us please first clarify the formulation used in the manuscript.
In the abstract, we write ``diverse probability laws and arbitrary impacts of the underlying uncertain parameters.''
So, the word \emph{arbitrary} belongs to \emph{impacts} and refers to the fact that the influence of the uncertain parameters is introduced into the framework as a ``black box,'' as it is unfolded in Sec.~III.
The formulation was ambiguous and has been modified in the revised version of the manuscript.

Despite the fact that we do not use the word \emph{arbitrary}, the theory of PC expansions can indeed be applied to arbitrary distributions (which is also emphasized in other papers such as [Vrudhula, 2006], the third paragraph on page 2007), and we would like to justify it now.
As the reviewer correctly noted, the output distribution is not known \apriori.
Therefore, whenever it comes to the construction of a PC expansion, one is typically guided by other factors in order to choose an adequate polynomial basis.
One such factor is the distributions of the independent random variables that parametrize the uncertainty quantification problem under consideration.
Then, many standard distributions directly correspond to certain families of orthogonal polynomials, as shown in Tab.~I.
When the reviewer says that PC expansions are not applicable to arbitrary distributions, he or she probably refers to the fact that not all probability distributions have such a direct correspondence with orthogonal polynomials.
It is true; however, one can always re-parametrize the problem by transforming one probability distribution into another, for example, to the one that does have a suitable polynomial basis.
Please refer to ``The Wiener-Askey Polynomial Chaos for Stochastic Differential Equations'' by D. Xiu \etal, namely, Sec.~6 titled ``Representation of Arbitrary Random Inputs.''
In addition, one does not have to perform such a re-parametrization and can construct custom orthogonal polynomials instead.
Please refer to ``Modeling Arbitrary Uncertainties Using Gram-Schmidt Polynomial Chaos'' by J. Witteveen \etal

We agree with the reviewer in the sense that the \emph{optimal} polynomial basis can indeed be unknown, and the expansion order can be high to reach the desired accuracy.
Nevertheless, PC expansions still hold, meaning that any second-order stochastic process admits such a representation, and this representation is convergent in the mean-square sense (the Cameron-Martin theorem).

An open question now is: Why do we use the word \emph{diverse} instead of \emph{arbitrary}?
The reason is to guard the paper.
Such ambitious statements always some formal assumptions underneath, which can formally invalidate these statements, even though the assumptions fail only for a few pathological cases.
For PC expansions, it is the second-order assumption (finite variance) mentioned in the last sentence of the previous paragraph and in Sec.~V-B of the manuscript.
However, most physical processes have finite variance as noted by D. Xiu in the aforementioned paper.
If the reviewer refers to this assumption, we apologize for our misunderstanding and remind that \emph{arbitrary} is not present in the manuscript.

\done{The formulation in the abstract has been clarified.}

\done{Possible solutions of non-standard distributions have been outlined in App.~C.}
\end{authors}

\begin{reviewer}
The paper did not address one important problems in thermal analysis: how the leakage and temperature interplay and its impacts on the statistical thermal analysis?
\end{reviewer}
\begin{authors}
We apologize, but this comment is not entirely clear for us.
Our leakage model is temperature aware, and, therefore, the interdependence between leakage and temperature is taken into consideration.
\end{authors}

\begin{reviewer}
The paper has many math notations. It will be nice to have table of all the math notations at the beginning of  the paper.
\end{reviewer}
\begin{authors}
Yes, we agree with the reviewer.
We considered the possibility of including such a table in the paper.
However, we decided to invest this paper space in thorough explanations and to remind the reader the notation on the fly when it is needed.
\end{authors}

\begin{reviewer}
In the experimental section:

The number of MC samples (10\^{}4) seems too big. For many practical problems, MC method with a few hundreds or thousands of samples will be good enough to get a good distributions.
\end{reviewer}
\begin{authors}
The choice of $\nsamples = 10^4$ for the comparison of the computational speed has three major aspects.

The first aspect is the experience from other studies.
For example, [Ghanta, 2006] analyzes power grids and uses $5 \times 10^3$ samples; [Shen, 2009] models the full-chip leakage power and uses $5 \times 10^4$ samples; [Chandra, 2010] performs system-level power analysis and uses $10^4$ samples; and [Cheng, 2011] studies across-wafer variations and uses $10^5$ samples.
Therefore, $\nsamples = 10^4$ is not an excess in our research area; moreover, one should take into consideration the fact heat transfer within electronic systems is one level above power.

The second aspect is the theoretical results given in [I. D\'{i}az-Emparanza, 2002].
For example (taken from an earlier publication of I. D\'{i}az-Emparanza), in order to approach the 0.05-probability tail of a probability distribution with confidence 99\% and accuracy 0.005, the estimated number of samples is roughly 12550.
This result, of course, is very general and applicable in any context.
However, it tells us the chosen value for $\nsamples$ makes sense.

The third aspect, which is the most important one, is our own experience and observations from the tables thoroughly discussed in Sec.~VII-A.
Moreover, as we write in the paper, we do believe that $\nsamples = 10^4$ is a lower bound.
However, we tried to avoid any bias towards the proposed technique, which could happen if we raised $\nsamples$ to $10^5$.

Let us also note the following.
All the prior studies listed in Sec.~II that perform comparisons with MC simulations in terms of accuracy and/or speed postulate their choices regarding the number of MC samples.
In contrast, we explain our choice and provide evidence to support it.
\end{authors}

\begin{reviewer}
Many pesudo MC methods actually have much faster convergent rate with much less number of samples actually.
\end{reviewer}
\begin{authors}
We agree with the reviewer that quasi-MC methods indeed can be faster than pure MC methods: the convergence rates are $O_1 = O((\log\nsamples)^\nvars / \nsamples)$ and $O_2 = O(1 / \sqrt{\nsamples})$ for the former and latter, respectively.
However, it was not the direction we wanted to put our efforts into: our major focus is at PC expansions, not at MC simulations.
We think it would not make much of a difference on the results reported in Tab. V and Tab. VI.
In addition, quasi-MC methods have their own problems.
For example, $O_1$ is smaller than $O_2$ only when $\nvars$ is small and $\nsamples$ is large.
In particular, already with $\nvars = 4$, the curve corresponding to $O_1$ is higher than the one of $O_2$ for $\nsamples$ from around 50 to $3 \times 10^6$.
In the experimental results, $\nvars$ goes up to 12.
\end{authors}

\begin{reviewer}
The table V and table VI look  bit weird.  It seems strange that the PC method just takes a few seconds, while MC method will takes hours. The speedups in the two tables  are even bigger than number of MC samples (10\^{}4) in many cases. This means that one PC-based computing take less than one MC sample run, which is impossible.  More explanations are needed.
\end{reviewer}
\begin{authors}
In order to draw adequate conclusions about the accuracy of our framework, the MC-based approach is supposed to be as accurate as possible, meaning that it should solve everything directly without any further simplifications or assumptions.
Therefore, as described at the beginning of Sec. VII, MC sampling solves the original system of differential equations directly using traditional numerical techniques.
In contrast, our framework employs the approximation described in App. A, which we also elaborate on in our previous publication listed in the references [8].
Consequently, the reported speedup of the proposed framework is due to two major factors: PC expansions \perse\ and the approximation of the thermal model.
And these two things are inseparable: our derivation process is tightly interconnected with the recurrence given in App. A.
Moreover, we have spent a great deal of time trying to construct a highly efficient algorithm for PC expansions.
From our experience, a straightforward implementation, blindly following the formulae commonly found in the literature, can end up being dramatically slow.
We are planning to present our solution process algorithmically as a part of our future publication.
\end{authors}
%
% To begin with, we would like to recall the assessment strategy utilized in the paper.
% Sec.~VII-A, titled ``Approximation Accuracy,'' provides a cross-comparison of the proposed technique with an MC-based approach wherein we consider a platform with $\nprocs = 4$ cores and a dynamic power profile with $\nsteps = 10^2$ steps.
% Our technique is given for a range of polynomial orders, $\pcorder$, while the MC-based approach is given for different numbers of samples, $\nsamples$.
% On top of it, we report the obtained results for three different correlation patterns; this is controlled by the $\eta$ parameter.
%
% Regarding the first one, the results presented in Tab.~II, Tab.~III, and Tab.~IV include the case with $\nsamples = 10^5$ and, therefore, show the errors of the proposed framework with respect to convergent estimates delivered by MC simulations.
% Since we suppose that this aspect is not the major concern of the reviewer, we do not discuss it here and kindly ask the reviewer to consider our response to the corresponding comment of Reviewer 5 wherein we justify solely the choice of $10^4$ samples.
%
%
% It can seen in Tab.~II, Tab.~III, and Tab.~IV that we present results for a range of configurations of the MC-based approach and a range of configurations of the proposed framework.
% One can also note the way the tables are discussed in Sec.~VII-A.
% Namely, we not only assess the proposed framework using MC sampling, but also assess MC sampling using our framework.
% Such a cross-assessment is possible since the accuracy of PC expansions and MC sampling increases as $\pcorder$ and $\nsamples$ grow.
% Therefore, the reader has a possibility to observe the consequences, \ie, the approximation errors, of choosing one approach or the other for a range of setups.
% This gives a wider picture of the relation between the two uncertainty quantification techniques and, eventually, leads a better appreciation of the proposed framework.
%
% Further, the assessment strategy utilized in the paper allows us to justify the choice of $10^4$ MC samples for the comparison of the computational speed in Sec.~VII-B.
% This justification is given in Sec.~VII-A where we elaborate on the three tables mentioned above.
% Moreover, as we reply to the fifth question of this reviewer, we do believe that $\nsamples = 10^4$ is a lower bound.
% However, we tried to avoid any bias towards the proposed technique, which could happen if we raised $\nsamples$ to $10^5$.
