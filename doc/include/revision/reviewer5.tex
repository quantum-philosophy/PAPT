\begin{reviewer}
The paper presented statistical thermal analysis for VLSI systems using the orthogonal polynomial  (or polynomial chaos, PC) based approaches. The idea is quite straight forward as the PCs was applied to represent the transient temperatures based on the PC representations of the power inputs for given thermal models. Some standard techniques such as KL and PCA for random variable reductions and orthonormalization was also used.

However, this paper has several major issues.

First is the novelty. Most of the techniques (PC, PCA etc.) have been applied to many analysis and modeling problems in the CAD areas as reviewed by the authors. The thermal analysis is not foundamentally different than the power grid analysis.  If we just look at the eqn(2), this statistical analysis problem is even a simplified version of power grid analysis problem where the A matrix is the function of have variational parameters.
\end{reviewer}
\begin{authors}
We understand and respect the reviewer's concern.
However, we do believe that the contribution of our work is sound, and we would like to explain it now in more detail.
Also, we kindly ask the reviewer to consider our answer to the next comment as a part of the answer to the current one.

First and the foremost,
\begin{enumerate}
  \item We are \emph{the first to formulate and solve the problem} of transient temperature analysis of electronic systems under the uncertainty due to process variation.
\end{enumerate}
The two temperature-driven works and their drawbacks are discussed in Sec.~II.
More precisely, [Juan, 2012] considers only steady-state temperature analysis, which is considerably simpler than transient, and, moreover, is heavily based on Gaussian distributions.
[Juan, 2011] also works only under the steady-state condition and, moreover, addresses only the maximal temperature.
Next,
\begin{enumerate}
  \setcounter{enumi}{1}
  \item We perform stochastic power analysis \emph{taking into consideration the interdependence between leakage and temperature}.
\end{enumerate}
From the perspective of physics, heat conduction within electronic systems and electrical conduction are two completely different phenomena.
From the perspective of mathematics, temperature analysis considered in our work and power-grid analysis considered in the prior work are completely different as the former is modeled with a system of general differential equations while the latter with a system of ordinary differential equations.
Temperature is one level above power, and, therefore, temperature analysis embraces the complexity associated with power analysis and justifiably increases it.
\begin{enumerate}
  \setcounter{enumi}{2}
  \item Not only claim but also show an application of PC expansions to non-Gaussian distributions.
  \item Propose a model to physically-constrained variations based on beta distributions.
  \item Show how to handle correlations for non-Gaussian parameters using the Nataf transformation.
  \item Flexibility due to the absence of any ad hoc expressions; generality.
  \item Systematic description of all the steps of the process with the corresponding solutions.
\end{enumerate}

First of all, both PC and PCA/KL are tools, and tools are \emph{designed/supposed} to be used to solve certain problems.
Therefore, the fact that the aforementioned techniques ``[\ldots] have been applied to many analysis and modeling problems in the CAD areas [\ldots]'' does not affect in any way the contribution of the present work.
As a matter of fact, PC expansions were developed in the first place to tackle stochastic differential equations, and it was in 1991 for the classical PC (``Stochastic Finite Elements: A Spectral Approach'' by R. Ghanem et al.) and in 2002 for the generalized PC (``The Wiener-Askey Polynomial Chaos for Stochastic Differential Equations'' by D. Xiu et al.).
Tools are there to help us, not to lie on a shelf.
Second, the reviewer claims that temperature analysis is not fundamentally different from power grid analysis, and he or she justifies this claim by saying that both phenomena are modeled via systems of first-order ordinary differential equations (ODEs), which look especially alike in their state-space forms (see Eq. (2)).
To begin with, myriads of dynamic systems are modeled using differential equations.
In numerical analysis, the solution process of such a model typically consists for a proper discretization of the state space, eventually leading to a system of ODEs, which, if possible, is then rewritten in the state-space form for convenience.
Therefore, following the logic of the comment, most of the physical processes are merely clones of each other, and, once one of them has been studied, the rest are out of any interest.
Of course, it is an exaggeration, but it makes our point.
Every process has its own particularities/nuances/features/traits, and heat dissipation is not an exception.
To conclude, the premises of the reviewer’s argument about the lack of novelty do not have a solid ground.
The manuscript presents a novel probabilistic framework for the transient temperature analysis under process variation and does make a considerable contribution to the research community.
We believe our argumentation is sufficient at this point; however, we would like to continue this discussion and emphasize some concrete differences from the prior work on power grid analysis [18].
One does not need to go too far to note that the way we treat the thermal system in App. A and the way we construct PC expansions in Sec. V-D drastically diverge from the solution process described in [18].
Further, our PC expansions are constructed using so-called non-intrusive projections, which provide a great flexibility in modeling various effects of the uncertain parameters (recall the discussion on ``black boxes''); in contrast, [18] employs the Galerkin method.
Turning to smaller details, the considered compositional correlation function is rather unique and has received positive feedback from the reviewers.
Next, we have not encountered in the literature the beta model for physically bounded quantities that we use in the illustrative example.
To summarize, all the discussions above advocate that our research makes a prominent contribution.
We sincerely hope that the manuscript will give birth to other temperature-related studies where, thanks to the accuracy and computational speed of the proposed framework, the presence of uncertainty will be seamlessly taken into account at little effort.

The uncertainty is injected into the proposed framework as a ``black box.''
Consequently, the user of our technique does not have to come up with various expressions describing the joint effects of the uncertain parameters, which, moreover, should be followed by a fitting procedure to the data from, \eg, SPICE simulations in order to determine all the coefficients of the chosen expression.
From our experience, it is considerably difficult to construct a good fit for the purpose of stochastic analysis since stochastic analysis tries to explore as large a portion of the probability space as possible, and, thus, this fit should capture very well broad ranges of the uncertain parameters.
In this regards, our preliminary research shown that the fits with polynomial of low orders (pure polynomials and rational functions w/o exponentiation) can introduce significant errors; it was also noted in [Shen, 2009].
Therefore, as mentioned in Sec.~VI-B, we simply utilize linear interpolation without any polynomial-based fitting, which the proposed framework readily allows us to do.
\end{authors}

\begin{reviewer}
Second is the motivation for the statistical thermal analysis problem. Many existing works mainly focus on the power or power grids as we can determine the chip yields once the powers or timing (due to power grids) distributions are known. If we limit the power to the given ranges, the temperature variations will be limited to a safe ranges. As a result, verify the temperature variations seems less important than verifying the power variations.  Also for multi-core and many-core architectures, the temperatures are highly loads dependent and the variations due to process variations maybe still secondary effects for a chip designers. Temperature is also environment and boundary conditions (fan speeds, liquid speed) dependent
\end{reviewer}
\begin{authors}
The fact that ``many existing works mainly focus on the power or power grids [\ldots]'' does not say anything about the importance of temperature.
In addition, if everybody was looking into the same direction, there would be no progress.
Regarding constraining power in order to squeeze temperature into safe ranges, we would like to respond with the words of another reviewer; they served a different purpose but are suitable in this context as well.
Such strategies ``[\ldots] can end up putting up a big enough guard band to design the system considering severe conditions.
Thus, they end up being quite conservative and over-designing their system.''
In other words, the more we know about the things we care about and the more immediate/direct this knowledge is, the more efficient and effective control we have over those important things.
Coming closer to the subject matter, it is temperature that causes damage, not power \perse.
Therefore, if we are to repress temperature, it is wiser to study this very temperature instead of power.
With all our respect to the reviewer, the claim that temperature is less important seem to us to be rather questionable.
Next, we agree with the reviewer that temperature is load dependent, which is equally well applies to power, and is influenced by environment.
However, we prefer to follow Caesar’s rule ``divide and conquer'': the focal point of this particular work is process variation, and we departure from a given workload, as stated in the problem formulation.
\end{authors}

\begin{reviewer}
In addition, the paper only consider the effective channel length, which makes the paper weak as different parameters may exhibits different correlations patterns (strong or weak). In case of weak correlations, the standard KL and PCA method will not work well as they can’t reduce many variables in this case. This may be the case even for the effective channel length. The authors only consider exponential correlation models.
\end{reviewer}
\begin{authors}
We pursued clarity of presentation and, therefore, focused one parameter in our illustrative example.
The choice of the effective channel length was a no-brainer as many studies show that it exhibits the largest deviation due to process variation and, therefore, constitutes the major concern.
We agree with the reviewer that different correlation patterns make the model order reduction procedure work differently, and it is important to understand.
For this particular reason, we presented the experimental results for various values of the $\eta$ parameter.
The goal was to illustrate and explain, not to give a broad overview of all possible correlation patterns.
It is worth mentioning that the number of considered parameters or the type of assumed correlations do not constrain the proposed framework; however, it is true that they affect the resulting complexity and lead to the curse of dimensionality as we emphasize in the paper.

First of all, as we emphasize at the end of Sec.~III and in Sec.~VIII, the proposed framework is not limited to any particular set of uncertain parameters or any particular way the considered parameters manifest themselves in the power model; thus, the designer has a great flexibility in taking various uncertain parameters into consideration.
Regarding the illustrative example and experimental results, we tried to give a clear demonstration of the proposed framework and, therefore, decided to focus on one parameter.
The choice of the effective channel length was straightforward as many studies show that it exhibits the largest deviation due to process variation and, therefore, constitutes the major concern.
\end{authors}

\begin{reviewer}
The claim that the proposed method can be applied to arbitrary distributions are not correct. As the author mentioned, that the resulting temperature or even the power distributions are not known a priori. As a result, we do not know which orthogonal polynomials we should use. This actually is the major limitations of the PC based method.
\end{reviewer}
\begin{authors}
First of all, we would like to note that the formulation used in the manuscript is ``diverse probability laws'' (see the abstract), not ``arbitrary.''
However, the theory of PC expansions can indeed be applied to arbitrary distributions as we shall justify shortly (which is also emphasized in other papers such as [Vrudhula, 2006], the third paragraph on page 2007).
As the reviewer correctly noted, the output distribution is not known \apriori.
Therefore, whenever it comes to the construction of a PC expansion, one is typically guided by other factors in order to choose an adequate polynomial basis.
One such factor is the distributions of the independent random variables that parametrize the uncertainty quantification problem under consideration.
Then, many standard distributions directly correspond to certain families of orthogonal polynomials, as shown in Tab.~I.
When the reviewer claims that PC expansions are not applicable to arbitrary distributions, he or she probably refers to the fact that not all probability distributions have such a direct correspondence with orthogonal polynomials.
It is true; however, one can always re-parametrize the problem by transforming one probability distribution into another, for example, to the one that does have a suitable polynomial basis.
Please refer to ``The Wiener-Askey Polynomial Chaos for Stochastic Differential Equations'' (the origin of the generalized PC) by D. Xiu \etal, namely, Sec.~6 titled ``Representation of Arbitrary Random Inputs.''
In addition, one does not have to perform this re-parametrization and can construct custom orthogonal polynomials instead.
Please refer to ``Modeling Arbitrary Uncertainties Using Gram-Schmidt Polynomial Chaos'' by J. Witteveen \etal\ We agree with the reviewer in the sense that the \emph{optimal} polynomial basis can indeed be unknown, and the expansion order, which is needed to reach the desired accuracy, can be high.
Nevertheless, PC expansions still hold, meaning that any second-order stochastic process admits such a representation, and this representation is convergent in the mean-square sense (the Cameron-Martin theorem, 1947).
\end{authors}

\begin{reviewer}
The paper did not address one important problems in thermal analysis: how the leakage and temperature interplay and its impacts on the statistical thermal analysis?
\end{reviewer}
\begin{authors}
We apologize, but this comment is not entirely clear for us.
Our leakage model is temperature aware, and, therefore, the interdependence between leakage and temperature is taken into consideration.
\end{authors}

\begin{reviewer}
The paper has many math notations. It will be nice to have table of all the math notations at the beginning of  the paper.
\end{reviewer}
\begin{authors}
Yes, we agree with the reviewer.
We considered the possibility of including such a table in the paper.
However, we decided to invest this paper space in thorough explanations and to remind the reader the notation on the fly when it is needed.
\end{authors}

\begin{reviewer}
In the experimental section:

The number of MC samples (10\^{}4) seems too big. For many practical problems, MC method with a few hundreds or thousands of samples will be good enough to get a good distributions.
\end{reviewer}
\begin{authors}
The choice of $\nsamples = 10^4$ for the comparison of the computational speed has three major aspects.

The first aspect is the experience from other studies.
For example, [Ghanta, 2006] analyzes power grids and uses $5 \times 10^3$ samples; [Shen, 2009] models the full-chip leakage power and uses $5 \times 10^4$ samples; [Chandra, 2010] performs system-level power analysis and uses $10^4$ samples; and [Cheng, 2011] studies across-wafer variations and uses $10^5$ samples.
Therefore, $\nsamples = 10^4$ is not an excess in our research area; moreover, one should take into consideration the fact heat transfer within electronic systems is one level above power.

The second aspect is the theoretical results given in [I. D\'{i}az-Emparanza, 2002].
For example (taken from an earlier publication of I. D\'{i}az-Emparanza), in order to approach the 0.05-probability tail of a probability distribution with confidence 99\% and accuracy 0.005, the estimated number of samples is roughly 12550.
This result, of course, is very general and applicable in any context.
However, it tells us the chosen value for $\nsamples$ makes sense.

The third aspect, which is the most important one, is our own experience and observations from the tables thoroughly discussed in Sec.~VII-A.
Moreover, as we reply to the fifth question of Reviewer 4, we do believe that $\nsamples = 10^4$ is a lower bound.
However, we tried to avoid any bias towards the proposed technique, which could happen if we raised $\nsamples$ to $10^5$.

Let us also note the following.
All the prior studies listed in Sec.~II that perform comparisons with MC simulations in terms of accuracy and/or speed postulate their choices regarding the number of MC samples.
In contrast, we explain our choice and provide evidence to support it.
\end{authors}

\begin{reviewer}
Many pesudo MC methods actually have much faster convergent rate with much less number of samples actually.
\end{reviewer}
\begin{authors}
We agree with the reviewer that quasi-MC methods indeed can be faster than pure MC methods: the convergence rates are $O_1 = O((\log\nsamples)^\nvars / \nsamples)$ and $O_2 = O(1 / \sqrt{\nsamples})$ for the former and latter, respectively.
However, it was not the direction we wanted to put our efforts into: our major focus is at PC expansions, not at MC simulations.
We think it would not make much of a difference on the results reported in Tab. V and Tab. VI.
In addition, quasi-MC methods have their own problems.
For example, $O_1$ is smaller than $O_2$ only when $\nvars$ is small and $\nsamples$ is large.
In particular, already with $\nvars = 4$, the curve corresponding to $O_1$ is higher than the one of $O_2$ for $\nsamples$ from around 50 to $3 \times 10^6$.
In the experimental results, $\nvars$ goes up to 12.
\end{authors}

\begin{reviewer}
The table V and table VI look  bit weird.  It seems strange that the PC method just takes a few seconds, while MC method will takes hours. The speedups in the two tables  are even bigger than number of MC samples (10\^{}4) in many cases. This means that one PC-based computing take less than one MC sample run, which is impossible.  More explanations are needed.
\end{reviewer}
\begin{authors}
In order to draw adequate conclusions about the accuracy of our framework, the MC-based approach is supposed to be as accurate as possible, meaning that it should solve everything directly without any further simplifications or assumptions.
Therefore, as described at the beginning of Sec. VII, MC sampling solves the original system of differential equations directly using traditional numerical techniques.
In contrast, our framework employs the approximation described in App. A, which we also elaborate on in our previous publication listed in the references [8].
Consequently, the reported speedup of the proposed framework is due to two major factors: PC expansions \perse\ and the approximation of the thermal model.
And these two things are inseparable: our derivation process is tightly interconnected with the recurrence given in App. A.
Moreover, we have spent a great deal of time trying to construct a highly efficient algorithm for PC expansions.
From our experience, a straightforward implementation, blindly following the formulae commonly found in the literature, can end up being dramatically slow.
We are planning to present our solution process algorithmically as a part of our future publication.
\end{authors}
%
% To begin with, we would like to recall the assessment strategy utilized in the paper.
% Sec.~VII-A, titled ``Approximation Accuracy,'' provides a cross-comparison of the proposed technique with an MC-based approach wherein we consider a platform with $\nprocs = 4$ cores and a dynamic power profile with $\nsteps = 10^2$ steps.
% Our technique is given for a range of polynomial orders, $\pcorder$, while the MC-based approach is given for different numbers of samples, $\nsamples$.
% On top of it, we report the obtained results for three different correlation patterns; this is controlled by the $\eta$ parameter.
%
% Regarding the first one, the results presented in Tab.~II, Tab.~III, and Tab.~IV include the case with $\nsamples = 10^5$ and, therefore, show the errors of the proposed framework with respect to convergent estimates delivered by MC simulations.
% Since we suppose that this aspect is not the major concern of the reviewer, we do not discuss it here and kindly ask the reviewer to consider our response to the corresponding comment of Reviewer 5 wherein we justify solely the choice of $10^4$ samples.
%
%
% It can seen in Tab.~II, Tab.~III, and Tab.~IV that we present results for a range of configurations of the MC-based approach and a range of configurations of the proposed framework.
% One can also note the way the tables are discussed in Sec.~VII-A.
% Namely, we not only assess the proposed framework using MC sampling, but also assess MC sampling using our framework.
% Such a cross-assessment is possible since the accuracy of PC expansions and MC sampling increases as $\pcorder$ and $\nsamples$ grow.
% Therefore, the reader has a possibility to observe the consequences, \ie, the approximation errors, of choosing one approach or the other for a range of setups.
% This gives a wider picture of the relation between the two uncertainty quantification techniques and, eventually, leads a better appreciation of the proposed framework.
%
% Further, the assessment strategy utilized in the paper allows us to justify the choice of $10^4$ MC samples for the comparison of the computational speed in Sec.~VII-B.
% This justification is given in Sec.~VII-A where we elaborate on the three tables mentioned above.
% Moreover, as we reply to the fifth question of this reviewer, we do believe that $\nsamples = 10^4$ is a lower bound.
% However, we tried to avoid any bias towards the proposed technique, which could happen if we raised $\nsamples$ to $10^5$.
