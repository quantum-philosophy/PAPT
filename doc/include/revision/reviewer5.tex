\begin{reviewer}
The paper presented statistical thermal analysis for VLSI systems using the orthogonal polynomial  (or polynomial chaos, PC) based approaches. The idea is quite straight forward as the PCs was applied to represent the transient temperatures based on the PC representations of the power inputs for given thermal models. Some standard techniques such as KL and PCA for random variable reductions and orthonormalization was also used.

However, this paper has several major issues.

\clabel{5}{1}
First is the novelty. Most of the techniques (PC, PCA etc.) have been applied to many analysis and modeling problems in the CAD areas as reviewed by the authors.
\end{reviewer}
\begin{authors}
We agree with the reviewer that the aforementioned techniques are known.
However, they are generic techniques, and applying a generic technique to a particular problem area, as we did in this paper, is a valid contribution as long as nobody has done it before.
Please consider our answers to the comments below.
\end{authors}

\begin{reviewer}
\clabel{5}{2}
The thermal analysis is not foundamentally different than the power grid analysis.  If we just look at the eqn(2), this statistical analysis problem is even a simplified version of power grid analysis problem where the A matrix is the function of have variational parameters.
\end{reviewer}
\begin{authors}
We believe that the contribution of our work is sound for the reasons specified as (R1) and (R2) in the following:
\begin{itemize}
  \item[(R1)] We are the first to formulate and solve the problem of transient power-temperature analysis of electronic systems under the uncertainty due to process variation.
\end{itemize}
The most relevant prior studies and their limitations are outlined in Sec.~II.
More specifically, [Juan, 2012] considers only static steady-state temperature analysis, which is considerably simpler than transient.
[Juan, 2011] relies on the static steady-state assumption as well and, moreover, addresses only the maximal temperature.
The rest of the prior works, namely, [Vrudhula, 2006] (a new reference), [Ghanta, 2006], [Bhardwaj, 2006], [Bhardwaj, 2008] (a new reference), [Shen, 2009], and [Chandra, 2010], are concerned with power analysis only.
Since there is no straightforward relation between power and temperature, the conclusions drawn with respect to power are not directly applicable to temperature.
Hence, the techniques solely focused on power analysis are of little help for the designer who is interested in temperature.

In addition, the proposed framework has the following distinguishing features:
\begin{itemize}
  \item[1.] Arbitrary (inter)dependencies between the power dissipation and operating temperature can be taken into consideration.
  In particular, the interdependence between leakage and temperature is systematically addressed.

  \item[2.] Arbitrary uncertain parameters can be readily analyzed.
  Our framework requires no modifications to be applied to various parameters since the manifestation of these parameters is introduced into the framework as a ``black box.''

  \item[3.] Arbitrary probability distributions of the uncertain parameters can be handled.\footnote{The applicability to arbitrary distributions is discussed later in our response to \cref{5}{5}.}

  \item[4.] The dependencies of the uncertain parameters are systematically addressed, and the requirements for the prior knowledge of these dependencies are mild.
  In particular, spatial correlations specified by a correlation matrix can be taken into account.
\end{itemize}

Further, the illustrative example given in the manuscript has the following features:
\begin{itemize}
  \item[5.] A general (non-Gaussian) setting is considered.
  In contract, those prior studies that, in theory, can handle arbitrary distributions illustrate their solutions using only Gaussian distributions.
  Non-Gaussian setups are more challenging than Gaussian, in particular, due to the fact that uncorrelated non-Gaussian variables are not independent in general.

  \item[6.] Physically-bounded variations are modeled using beta distributions, which can capture the nature of such variations better.
  We have not encountered such a model in the literature; however, it has its own right due to the boundedness of the support; see Sec.~VII.

  \item[7.] Spatial correlations are modeled using a composition of two correlation functions, which is rather unique.
  We combine the Ornstein-Uhlenbeck kernel, which is omnipresent in the literature, with the squared-exponential kernel, which has not received much attention.

  \item[8.] The Nataf transformation is demonstrated.
  This transformation is not known sufficiently well in our research area.
  The paper brings attention to this powerful technique.
\end{itemize}

The above-mentioned features of the proposed framework and illustrative example will be further detailed under the following argument supporting the soundness of our contribution.
We shall refer to these features as \feature{1}--\feature{8}.

\begin{itemize}
  \item[(R2)] The problem of transient power-temperature analysis under process variation cannot be solved in a straightforward manner using the state-of-the-art techniques.
\end{itemize}
Let us begin with the temperature-targeted studies, namely, [Juan, 2011] and [Juan, 2012].
Transient temperature analysis is concerned with the dynamics of the electronic system under consideration and is governed by a system of differential equations.
On the other hand, static steady-state temperature analysis is narrowed to the steady state of the system and is based on a system of linear equations.
Both [Juan, 2011] and [Juan, 2012] are based on the static steady-state assumption and, therefore, cannot be trivially applied to solve the problem formulated in our paper.

Let us now turn to the power-targeted studies.
In this case, the prior techniques can be classified into two groups.
The first group aims at the estimation of the total leakage power and includes [Bhardwaj, 2006], [Bhardwaj, 2008], and [Shen, 2009].
The second group can be used to analyze transient power and includes [Vrudhula, 2006], [Ghanta, 2006], and [Chandra, 2010].
Similar to static steady-state temperature analysis, the first group is not concerned with any dynamics of the system, and a potential extension to transient processes is not straightforward.

The second group techniques, which is related to transient power, can be classified even further.
[Vrudhula, 2006] and [Ghanta, 2006] are based on spectral methods, namely, PC expansions and the KL decomposition, whereas [Chandra, 2010] is based on MC simulations.
[Chandra, 2010] provides a temperature-aware technique for power analysis.
However, due to the limitations induced by the MC-based core of this technique, the temperature dependency is taken into account in a rather crude manner \via\ precomputed temperature traces.
Therefore, a straightforward application of the approach in [Chandra, 2010] to the problem considered in our work is not possible.

Two prior works are left to discuss, namely, [Vrudhula, 2006] and [Ghanta, 2006].
[Ghanta, 2006] is the paper that the reviewer refers to in the comment, and the reviewer is right that heat conduction in our work and electrical conduction in [Ghanta, 2006] are both modeled \via\ systems of first-order ordinary differential equations (ODEs).
Taking this concern into consideration, we have also included [Vrudhula, 2006] in the manuscript as it contains another study based on a system of differential equations; more precisely, the subject of [Vrudhula, 2006] is the response of interconnect networks.
In what follows, we shall demonstrate that the solutions in [Vrudhula, 2006] and [Ghanta, 2006] cannot be trivially applied to the problem considered in our work.

We begin with \feature{1} of the proposed framework given earlier.
The dependency of power on temperature, taken into account in our approach, implies that the input vector of power in Eq.~(2) is a nonlinear function of the output vector of temperature.
Hence, the system of ODEs considered in our work is nonlinear.
Meanwhile, the systems of ODEs in [Vrudhula, 2006] and [Ghanta, 2006] are linear.
The presence of a feedback loop makes the problem more challenging and requires a different solution strategy.
This aspect is especially urgent in the present of process variation: the source/power term on the right-hand side of Eq.~(2a) is uncertain not only due to the direct dependency on the uncertain parameters, but also due to the influence of temperature, which is another uncertain quantity as it depends on power.
It can be seen in Sec.~VI that our step-wise solution drastically differs from those described in [Vrudhula, 2006] and [Ghanta, 2006], and it is very efficient as we discuss in our response to \cref{5}{10}.

With respect to \feature{1}, it is worth emphasizing that all the prior works based on spectral methods for uncertainty quantification, \ie, [Vrudhula, 2006], [Ghanta, 2006], [Bhardwaj, 2006], [Bhardwaj, 2008], and [Shen, 2009], ignore the leakage-temperature interdependence.
This interdependence, however, is of a tremendous importance, which is well known and requires no introduction; see, \eg, [Liu, 2007] and [Srivastava, 2010].

Let us turn to \feature{2}.
The developments given in [Vrudhula, 2006] and [Ghanta, 2006] are tailored for particular sets of uncertain parameters.
They rely on specific expressions, describing how these parameters influence the system, and these expression are tightly entangled with the overall solution process.
As noted by the authors of [Vrudhula, 2006], their analysis of interconnects is not limited to any particular formula modeling the influence of the internal independent variables, which is true.
However, a potential extension is not straightforward; the same applies to [Ghanta, 2006].
The reason for this is the use of so-called intrusive Galerkin projections for the computation of the coefficient of PC expansions.
This means that the user of [Vrudhula, 2006] and [Ghanta, 2006] has to adapt their solutions to every problem individually.
In contrast, as mentioned in Sec.~III and further elaborated on in Sec.~V-B, our approach keeps the uncertain parameters inside a ``black box,'' which can be replaced with another box at no effort.
This type of projections is called non-intrusive [Eldred, 2008].
The user of our technique, of course, should provide such a box; however, one has much more freedom to do so.

In particular, \feature{2} implies that the user does not have to come up with artificial expressions describing the joint effects of the uncertain parameters and to perform curve fitting procedures to the data from SPICE simulations (see, \eg, [Bhardwaj, 2008]) in order to determine the coefficients of these expressions.
From our experience, it is rather difficult to construct a good fit for the purpose of stochastic analysis since stochastic analysis tries to explore as large a portion of the probability space as possible, and, thus, this fit should capture very well broad ranges of the parameters.
In this regard, our preliminary research has shown that the fits with polynomials of low orders (pure polynomials and rational functions w/o exponentiation) can introduce significant errors, which is also noted in [Shen, 2009].
As mentioned in Sec.~VI-B, in our experiments, we utilize linear interpolation without any polynomial-based fitting, which the proposed framework readily allows us to do.

Regarding \feature{3}, [Vrudhula, 2006] states that the proposed methodology can be applied to arbitrary distributions, which is true.
However, the only illustrated case is Gaussian.\footnote{The authors also discuss log-normal parameters; however, this case is reduced to Gaussian due to the natural relation between the two families of distributions.}
In a non-Gaussian setting with dependencies, PC expansions are not straightforward.
The reason is that mutual independence is essential for PC expansions, and linear transformations, removing correlations, in general, do not yield independence.
On the other hand, [Ghanta, 2006] is tightened to Gaussian distributions, which are critical for the development in that paper (the same in [Bhardwaj, 2006] and [Bhardwaj, 2008]).
In contrast, the proposed framework is exemplified using a general setup (\feature{5}, \feature{6}, and \feature{8}).

With respect to \feature{4}, the methodology described in Sec.~V in [Vrudhula, 2006] does not address the case when the uncertain parameters are dependent; a potential solution is mentioned only in the context of Gaussian distributions.
On the other hand, [Ghanta, 2006] is founded on the basis of the continuous version of the KL decomposition and, therefore, assumes that the correlation function of the uncertain parameters is known (the same in [Bhardwaj, 2006] and [Bhardwaj, 2008]).
Such an assumption, however, is impractical, which is also noted by [Vrudhula, 2006].
In reality, it can be difficult to make a justifiable choice and tune such a correlation function.
As described in Sec.~V-A and Sec.~VI-A, the knowledge of a correlation matrix is sufficient to characterize dependencies for our approach (\feature{8}).
Such a matrix can readily be estimated from measurements and, thus, is a more probable input to stochastic analyses.
Note that we also assume a correlation function, Eq.~(13).
However, although it is rather unique and captures well certain features inherent to the fabrication process (\feature{7}), we use this function with the only purpose of constructing correlation matrices for our experiments.

Taking into consideration the argumentation given above, we conclude that our research makes a prominent contribution: it solves the problem of transient temperature analysis under process variation for the first time, and none of the state-of-the-art techniques can be applied in a straightforward manner to tackle this problem.

\begin{actions}
  \action{The prior works have been described in more detail in Sec.~II.}
  \action{The nonlinearity of the thermal system has been emphasized in Sec.~V-C.}
  \action{The benefits of non-intrusive PC expansions have been emphasized at the end of Sec.~V-D.}
  \action{The benefits of working with correlation matrices instead of correlation functions has been explained in Sec.~VI.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{5}{3}
Second is the motivation for the statistical thermal analysis problem. Many existing works mainly focus on the power or power grids as we can determine the chip yields once the powers or timing (due to power grids) distributions are known. If we limit the power to the given ranges, the temperature variations will be limited to a safe ranges. As a result, verify the temperature variations seems less important than verifying the power variations. Also for multi-core and many-core architectures, the temperatures are highly loads dependent and the variations due to process variations maybe still secondary effects for a chip designers. Temperature is also environment and boundary conditions (fan speeds, liquid speed) dependent
\end{reviewer}
\begin{authors}
First, we believe that power analysis and temperature analysis are inseparable.
The reason is the ever so important interdependence between temperature and leakage: high temperatures lead to the increase of the leakage power, and the leakage power strikes back by cranking up the dissipation of heat.
Process variation, which has been rapidly increasing over the last decade due to technology scaling, magnifies the aforementioned concern even further.
Therefore, stochastic power analysis has to be accompanied by stochastic temperature analysis.
The proposed framework is self-contained from this perspective, which differentiates us from the prior works as discussed in our previous answers.

Second, the immediate cause of various reliability issues (\eg, time-dependent dielectric breakdown, electro-migration, and thermal-cycling fatigue) is temperature, not power \perse.
Moreover, for certain failure mechanisms, the failure rate has an exponential dependency on the operating temperature [Juan, 2012].
In order to alleviate such issues, the designer indeed can try to constrain power and hope that temperature will get squeezed into safe ranges.
In this regard, we would like to respond with the words of Reviewer 3 (\cref{3}{1}):
Such strategies ``[\ldots] can end up putting up a big enough guard band to design the system considering severe conditions.
Thus, they end up being quite conservative and over-designing their system.''
In other words, the more we know about the quantities we care about and the more immediate/direct this knowledge is, the more efficient and effective control we have over those important quantities.
Therefore, if we are interested in temperature, it is wiser to study this very temperature instead of power.
In addition, since power does not translate to temperature in a straightforward manner, the well-established reliability models based on temperature will be excluded from the arsenal of the designer when only stochastic power analysis is at the designer's disposal.
This implies, for example, that various temperature-dependent design-space-exploration procedures will not be possible to undertake.

We agree with the reviewer that temperature is load dependent (which applies to power equally well) and can be influenced by other conditions.
The focal point of this paper is process variation, and the aforementioned factors are left for the future work.
However, it is worth noting that the effect of process variation on temperature is substantial, in particular, due the leakage-temperature interdependence discussed earlier.
This is also emphasized in other temperature-related studies.
For instance, [Juan, 2011] gives an example wherein the maximal temperature fluctuates within a band of around $20^\circ$C, depending on the severity of process variation.
Our example in Fig.~1 indicates a similar sensitivity of temperature to process variation.

\begin{actions}
  \action{Our motivation has been explained in more detail in Sec.~I.}
  \action{It has been noted that process variation is not the only source of uncertainty in Sec.~IV.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{5}{4}
In addition, the paper only consider the effective channel length, which makes the paper weak as different parameters may exhibits different correlations patterns (strong or weak). In case of weak correlations, the standard KL and PCA method will not work well as they canâ€™t reduce many variables in this case. This may be the case even for the effective channel length. The authors only consider exponential correlation models.
\end{reviewer}
\begin{authors}
We would like to note first that the proposed framework does not pose any constraints on the considered uncertain parameters $\vU(\o)$ including their correlations.

The reviewer is absolutely right that different correlation patterns make the model order reduction procedure work differently.
As a result, two sets of initial uncertain parameters $\vU'(\o)$ and $\vU''(\o)$ of the same cardinality but with different correlation structures can end up in two sets of independent random variables $\vZ'(\o)$ and $\vZ''(\o)$ whose cardinalities arbitrary differ from each other.
Therefore, it is difficult to make a connection between the dimensionality of $\vU(\o)$ with the dimensionality of $\vZ(\o)$.
At the same time, the applicability of our approach, which is the main concern of the comment, is essentially dictated by the dimensionality of $\vZ(\o)$, $\nvars$.
Therefore, $\vZ(\o)$ is a better indicator of this applicability than $\vU(\o)$, and the latter can be even misleading.
In order to illustrate this concern, we would like to include here the following two paragraphs taken from our response to \cref{3}{7}.

Suppose, apart from the effective channel length $L$, we take into account one additional parameter denoted by $W$.
Then, the first question to ask is whether $L$ and $W$ are independent.
Assume first they are.
The second question to ask is whether $W$ has local variations.
If it does not, $\nvars$ will be increased by one, relative to the case wherein only $L$ is considered.
If it does, the third question to ask is: What is the correlation patter inherent to $W$?
For instance, if the pattern is the same as for $L$, $\nvars$ will be doubled in the end.
In general, the answer depends on how strong/weak the assumed spatial correlations of $W$ are.
Coming back to the very first question we asked, suppose $L$ and $W$ are now correlated.
In this case, the follow-up questions are the same as before; however, $\nvars$ will generally be smaller than in the first scenario due to additional possibilities for model order reduction.
As we can see, it is not straightforward to relate the considered uncertain parameters $\vU(\o)$ with the resulting independent random variables $\vZ(\o)$ and, thus, with the complexity of the further analysis.

To summarize, we believe that the foremost quantity to pay attention to is the dimensionality of $\vZ(\o)$, $\nvars$, which, of course, can increase with additional uncertain parameters.
The column of Tab.~V titled ``$\nvars$'' serves this exact purpose: it demonstrates how the computational speed of the proposed framework scales with respect to the number of independent parameters.
The fact that there is another variable in Tab.~V, namely, the number of processing elements, $\nprocs$, does not affect the corresponding conclusions since, as mentioned above, the complexity is dictated by the stochastic dimensionality of the problem.
Regarding accuracy, we observed that fourth-order PC expansions (used to obtain the results in Tab.~V) were sufficiently accurate for the purpose of temperature analysis considered in this paper.
Note also that this order is higher than the one used for power in the PC-related literature discussed in Sec.~II: typically, it is the second order (see, \eg, [Bhardwaj, 2008] and [Shen, 2009]).
Therefore, following a similar reasoning as in the previous paragraph, the information provided by Tab.~V allows the reader to draw conclusions about the performance of the proposed framework when additional parameters are considered.

Let us continue our discussion here and emphasize that the effect of correlation patterns is indeed important; we are in a perfect alignment with the reviewer in this regard.
For this particular reason, the experimental results are given for various values of the $\eta$ parameter, which controls these patterns and, thus, illustrates their influence.
However, the goal was to illustrate and explain rather than to give a broad overview of possible correlation structures.
Therefore, we decided to take the Ornstein-Uhlenbeck kernel, which is used in many papers (see, \eg, [Ghanta 2006], [Bhardwaj, 2006], [Bhardwaj, 2008], and [Shen, 2009]) and is used alone, and combine it with the squared-exponential kernel, which has not received much attention in the literature.
In some cases, the lack of attention to the second kernel can be explained by the lack of an analytical solution for this correlation function with respect to the continuous KL decomposition (utilized in [Ghanta 2006], [Bhardwaj, 2006], and [Bhardwaj, 2008]).
From this perspective, our exploration of correlation patterns is more comprehensive than the ones in the prior works.

Let us also comment separately on the choice of the effective channel length.
As with correlations, the goal was to give a clear illustration, and, therefore, we decided to pick one parameter.
Our choice was guided by the reasoning given in [Juan, 2011, 2012].
To elaborate, the two main components of the total leakage current are the subthreshold and gate leakage currents.
The former constitutes the major concern since the importance of the latter has been recently reduced with the introduction of high-k dielectrics.
Then, apart from temperature, the subthreshold leakage current has a strong dependency on the effective channel length and threshold voltage, which are severely deteriorated by process variation.
The threshold voltage is most sensitive to the effective channel length and gate oxide thickness with the latter being the weakest one [Shen, 2010].
Thus, the effective channel length was chosen.
Consequently, the proposed framework is exemplified on arguably the most crucial process parameter.

\begin{actions}
  \action{The applicability of our approach with respect to $\nvars$ has been discussed in Sec.~VII-B.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{5}{5}
The claim that the proposed method can be applied to arbitrary distributions are not correct. As the author mentioned, that the resulting temperature or even the power distributions are not known a priori. As a result, we do not know which orthogonal polynomials we should use. This actually is the major limitations of the PC based method.
\end{reviewer}
\begin{authors}
First, we would like to clarify the formulation used in the manuscript.
In the abstract, we write ``diverse probability laws and arbitrary impacts of the underlying uncertain parameters.''
So, the word \emph{arbitrary} belongs to \emph{impacts} and refers to the fact that the influence of the uncertain parameters is introduced into the framework as a ``black box,'' as it is unfolded in Sec.~III.
The formulation was ambiguous and has been modified in the revised version of the manuscript.

Despite the fact that we do not use the word \emph{arbitrary}, the theory of PC expansions can indeed be applied to arbitrary distributions (which is also emphasized in other papers such as [Vrudhula, 2006]), and we would like to justify it now.

Consider a stochastic system.
The output of the system is a stochastic process.
By the Cameron-Martin theorem established in the 1940s, the space of square-integrable (finite variance) stochastic processes is a separable Hilbert space wherein the countable orthonormal basis is formed by the Hermite polynomials.
Therefore, an arbitrary square-integrable stochastic process admits an expansion, convergent in the mean-square sense, with respect to the basis.
In 2002, this result was generalized to the hypergeometric orthogonal polynomials from the Askey scheme in ``The Wiener-Askey Polynomial Chaos for Stochastic Differential Equations'' by D. Xiu \etal

Let us now look at the system from a more practical perspective.
The system is stochastic due to its internal dependence on a set of uncertain parameters $\vU(\o)$.
As the reviewer correctly noted, the output distribution is not known \apriori.
Therefore, whenever it comes to the construction of a PC expansion, one is typically guided by other factors in order to choose an adequate polynomial basis.
One such factor is the distributions of the independent random variables $\vZ(\o)$, which are either $\vU(\o)$ in the absence of dependencies or the result of the preprocessing stage in the presence of dependencies.
Then, many standard distributions directly correspond to certain families of orthogonal polynomials from the Askey scheme.
When the reviewer says that PC expansions are not applicable to arbitrary distributions, the reviewer probably refers to the fact that not all probability distributions have such a direct correspondence with orthogonal polynomials, which is true.
However, one can always re-parametrize the problem by transforming one probability distribution into another, for example, to the one that does have a suitable polynomial basis.
Please refer to the above paper by D. Xiu \etal, more precisely, to Sec.~6 titled ``Representation of Arbitrary Random Inputs.''
In addition, one does not have to perform such a re-parametrization and can construct custom orthogonal polynomials instead.
Please refer to ``Modeling Arbitrary Uncertainties Using Gram-Schmidt Polynomial Chaos'' by J. Witteveen \etal

We agree with the reviewer in the sense that the \emph{optimal} polynomial basis can indeed be unknown, and the expansion order can be rather high to reach the desired level of accuracy.
Nevertheless, PC expansions are still legitimate and converge in the mean-square sense.

Let us now explain why we prefer to use the word \emph{diverse} instead of \emph{arbitrary}.
Such ambitious statements as ``applicable to arbitrary distributions'' almost always have some formal assumptions underneath that can formally invalidate these statements, even though the assumptions fail only for a few pathological cases.
For PC expansions, it is the finite-variance assumption mentioned above and in Sec.~V-B of the manuscript.
However, most physical processes have finite variance; see the above paper by D. Xiu \etal{}
If the reviewer refers to this assumption, we apologize for our misunderstanding and remind that \emph{arbitrary} is not present in the manuscript.

\begin{actions}
  \action{The formulation in the abstract has been revised.}
  \action{Possible solutions for general distributions have been outlined in App.~C.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{5}{6}
The paper did not address one important problems in thermal analysis: how the leakage and temperature interplay and its impacts on the statistical thermal analysis?
\end{reviewer}
\begin{authors}
As mentioned in our previous answers and, in particular, in the answer to \cref{5}{2}, the stochastic power-temperature analysis presented in the paper does take into consideration the interdependence between leakage and temperature.
The general dependency of power on temperature can be seen in Eq.~(1) and is noted at the end of Sec.~V-D3.
The leakage-temperature interdependence is mentioned at the end of Sec.~V-B and in Sec.~VI-B.
Therefore, all the experimental results explore this important aspect.

In the revised manuscript, we have drawn more attention to this interdependency.

\begin{actions}
  \action{The importance of the leakage-temperature interdependence has been mentioned in Sec.~I.}
  \action{The lack of considering this interdependence by the prior works has been emphasized in Sec.~II.}
  \action{The fact that our framework accounts for the interdependence has been noted in Sec.~III.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{5}{7}
The paper has many math notations. It will be nice to have table of all the math notations at the beginning of  the paper.
\end{reviewer}
\begin{authors}
We agree with the reviewer.
Such a table has been introduced on page 2.
In order to make room for the table, we remove Tab.~1---it was illustrating the correspondence between probability distributions and polynomial bases---since this information can be easily found in the cited literature.
Also, due to the shortage of space, we had to constrain the list of notations and, thus, cherry-picked only the most relevant notations from our perspective.

\begin{actions}
  \action{A table with the main notations has been introduced on page 2.}
\end{actions}
\end{authors}

\begin{reviewer}
In the experimental section:

\clabel{5}{8}
The number of MC samples (10\^{}4) seems too big. For many practical problems, MC method with a few hundreds or thousands of samples will be good enough to get a good distributions.
\end{reviewer}
\begin{authors}
The choice of $\nsamples = 10^4$ for the comparison of the computational speed in Sec.~VII-B is based on the three major aspects given below and also in our response to \cref{4}{3}.

The first aspect is the experience from other studies.
For example, [Ghanta, 2006] analyzes power grids and uses $5 \times 10^3$ samples; [Bhardwaj, 2008] studies the full-chip leakage power and uses $10^4$ samples; [Shen, 2009] is also concerned with the full-chip leakage power and uses $5 \times 10^4$ samples; [Chandra, 2010] performs system-level power analysis and uses $10^4$ samples; and [Cheng, 2011] studies across-wafer variations and uses $10^5$ samples.
Therefore, we conclude that our $\nsamples = 10^4$ is not an excess in this research area.
One should also take into consideration the fact that heat transfer is one level above power, and, therefore, the complexity of temperature analysis can reasonably be higher than the one of power analysis.
For this reason, the number of MC samples can also increase to retain the same accuracy.

The second aspect is the theoretical results given in [I. D\'{i}az-Emparanza, 2002].
Let us give an example taken from an earlier publication of I. D\'{i}az-Emparanza: in order to approach the 0.05-probability tail of a probability distribution with confidence 99\% and accuracy 0.005, the estimated number of samples is roughly 12550.
This result, of course, is very general and applicable in many contexts.
However, it tells us that the chosen value for $\nsamples$ makes sense.

The third aspect, which is the most important one, is our own experience and observations from the tables thoroughly discussed in Sec.~VII-A.
Moreover, as we write in the paper, we believe that $\nsamples = 10^4$ is a lower bound, which is also related to \cref{4}{5}.
At the same time, we tried to avoid any bias towards our technique, which could happen if we raised $\nsamples$ to $10^5$.
Therefore, even if the choice of $\nsamples = 10^4$ is biased, it is biased in favor of the MC-based technique.

In addition, it is worth emphasize that all the prior studies listed in Sec.~II which perform comparisons with MC simulations in terms of accuracy and/or speed, namely, [Vrudhula, 2006], [Ghanta, 2006], [Bhardwaj, 2008], [Shen, 2009], [Chandra, 2010], [Juan, 2011], and [Juan, 2012], postulate their choices regarding the number of MC samples.\footnote{One prior work, namely, [Bhardwaj, 2006], is not listed here as there is no comparison with MC sampling in that work.}
In contrast, we explain our choice and provide evidence to support it.

\begin{actions}
  \action{The discussion about the choice of $\nsamples = 10^4$ has been reformulated, moved from Sec.~VII-A to Sec.~VII-B, and supported with additional citations.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{5}{9}
Many pesudo MC methods actually have much faster convergent rate with much less number of samples actually.
\end{reviewer}
\begin{authors}
We agree with the reviewer that there are other sampling techniques, such as Latin hypercube and quasi-MC sampling, which try to improve upon the convergence properties of the classical MC sampling.
However, their applicability is often limited due to additional restrictions posed by the design of these methods [Xiu, 2010].
For example, quasi-MC sampling can indeed be faster than pure MC sampling since the convergence rates are $\O_1 = \O((\log\nsamples)^\nvars / \nsamples)$ and $\O_2 = \O(1 / \sqrt{\nsamples})$ for the former and latter, respectively.
However, $\O_1$ is smaller than $\O_2$ only when $\nvars$ is small and $\nsamples$ is large.
In particular, already with $\nvars = 4$, the curve corresponding to $\O_1$ is higher than the one of $\O_2$ for $\nsamples$ from around 50 to $3 \times 10^6$.
In our experimental results, $\nvars$ goes up to 12.

Let us also note that MC sampling was not the direction we wanted to put our efforts into: our major focus is at PC expansions.

\begin{actions}
  \action{Alternative sampling methods have been mentioned in Sec.~II.}
\end{actions}
\end{authors}

\begin{reviewer}
\clabel{5}{10}
The table V and table VI look  bit weird.  It seems strange that the PC method just takes a few seconds, while MC method will takes hours. The speedups in the two tables  are even bigger than number of MC samples (10\^{}4) in many cases. This means that one PC-based computing take less than one MC sample run, which is impossible.  More explanations are needed.
\end{reviewer}
\begin{authors}
Thank you for the excellent observation.
Let us explain how this speedup has been achieved.

First of all, recall that we utilize quadrature-based non-intrusive projections for the calculation of the coefficients of PC expansions.
The reviewer is right that, if we were to construct such non-intrusive PC expansions in the traditional way, as it is usually presented in the corresponding literature (see, \eg, [Eldred, 2008]), the proposed framework could not be faster that one MC simulation.
To be precise, the speedup would be exactly the ratio between the number of MC samples $\nsamples$ and the number of quadrature nodes $\qdorder$ utilized for non-intrusive projections inside our approach; we would not need to perform any actual time measurements.

To elaborate, the beauty of non-intrusive projections is in the fact that well-established deterministic codes require no modifications in order to be analyzed from the stochastic perspective when the input to these codes becomes stochastic.
Therefore, when one has a deterministic solver for some problem and wishes to quantify this problem under a stochastic input, one takes this solver \emph{as is} and plugs it into an appropriate uncertainty quantification framework, which calls this code a number of times with different (deterministic, fixed) inputs according to a certain strategy.
In our case, this strategy is the chosen quadrature rule.
Therefore, as many nodes are in the rule, as many times the solver will be called.
Consequently, if we followed the procedure described above, our approach would be racing with the MC-based approach only in terms of the number of calls to a numerical solver of the system of differential equations given in Eq.~(2) (in its deterministic form, \ie, when $\o$ or, equivalently, $\vU(\o)$ is fixed).

However, our solution diverges from the traditional construction of non-intrusive PC expansions.
We go inside the solver and perform PC expansions in a step-wise manner following the recurrence given in Eq.~(3), which eventually leads to the recurrence for the PC coefficients in Eq.~(7).
The advantages of such an approach are summarized in the last two paragraphs of Sec.~VI-D (the very last one has been added in the revised version of the manuscript).
As described in Sec.~VI-C and App.~A, the derivation of this organic part of out framework is done under the assumption that the granularity of power/temperature profiles is sufficiently small such that the power consumption stays approximately constant within one time interval.

On the other hand, the MC-based approach is a brute-force technique.
The accuracy of this approach is not compromised by any additional assumptions: there is no model order reduction inside and the thermal system is solved directly using traditional numerical techniques.
In our case, it is Runge-Kutta formulae, as described at the beginning of Sec.~VII.

Consequently, the reported speedups of the proposed framework are due to two major factors: PC expansions \perse\ and the recurrence-based solution process of the thermal model.

For a better intuition of the computational speed of the proposed framework, we kindly refer the reviewer to our answer to \cref{4}{1a} wherein an analysis of the time complexity is given.

\begin{actions}
  \action{The two contributors to the speedup of the proposed framework have been highlighted in Sec.~VII-B.}
\end{actions}
\end{authors}
