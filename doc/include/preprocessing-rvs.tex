As mentioned in \sref{uncertain-parameters}, the uncertain parameters $\vU(\o)$ should be preprocessed to extract a set of mutually independent r.v.'s $\vZ(\o)$. It is generally accepted that the uncertainties of the channel length caused by the process variation are distributed normally \cite{juan2011, juan2012, srivastava2010}, i.e., $\vU(\o) \sim \normal{\vZero}{\mCov_\vU}$, therefore, according to the strategy outlined in \sref{uncertain-parameters}, an appropriate linear transformation can be employed.

Global variations are typically assumed to be uncorrelated with respect to local variations, which has no loss of generality. Therefore, we can treat these two parts separately. $\gLeff(\o)$ can be simply normalized as $\gLeff(\o) = \std_\gLeff \gZ(\o)$ where $\gZ(\o) \sim \normal{0}{1}$. For the local variations, denote $\vlLeff(\o) = \vec{\lLeff_1(\o), \dotsc, \lLeff_\cores(\o)} \in \real^\cores$ and $\oCov{\vlLeff(\o)} = \mCov_\vlLeff$\footnote{The covariance matrix $\mCov_\vlLeff$ is generated with respect to \eref{correlation-matrix}}. Since any covariance matrix is necessarily a real, symmetric matrix, it admits the eigenvalue factorization \cite{press2007} of the form $\mCov = \m{V} \m{\Lambda} \m{V}^T$ where $\m{V}$ and $\m{\Lambda}$ are an orthogonal matrix of the eigenvectors and a diagonal matrix of the eigenvalues of $\mCov$, respectively. Consequently, $\vlLeff(\o)$ can be decomposed as $\vlLeff(\o) = \m{V} \m{\Lambda}^\ifrac{1}{2} \vlZ(\o)$ where $\vlZ(\o) \sim \normal{\vZero}{\mI}$. The components of $\vlZ(\o)$ are uncorrelated and, since Gaussian, independent. Finally, $\gZ(\o)$ is appended to $\vlZ(\o)$ forming the desired vector of centered, normalized, and mutually independent uncertain parameters $\vZ(\o) \in \real^\vars$ where $\vars = \cores + 1$.

The number of stochastic dimensions $\vars$ directly affects the overall computational cost since it increases the number of terms $\pcterms$ (\eref{pc-terms}) in the PC expansion and the difficulty of inner products for the corresponding coefficients (\eref{pc-coefficients}). Therefore, we should consider a possibility of model order reduction before constructing a PC expansion. The intuition is that, due to the existing correlations between r.v.'s, some of them can be harmlessly replaced by linear combinations of the rest. One way to reveal these redundancies is to analyze the diagonal matrix of eigenvalues $\m{\Lambda} = \diag{ \lambda_i }$, which we obtained in the previous paragraph. Assume $\lambda_i$, $\forall i$, are arranged in the non-increasing order and let $\bar{\lambda}_i = \lambda_i / \sum_j \lambda_j$. Gradually summing up the arranged and normalized eigenvalues $\bar{\lambda}_i$, we can identify a subset of them, which has the cumulative sum greater than a certain threshold $\Lth$. When $\Lth$ is sufficiently high (close to 1), the rest of the eigenvalues and the corresponding eigenvectors can be dropped as being insignificant reducing the number of stochastic dimensions $\vars$.

The whole procedure described in this subsection is known as the Principal Component Analysis (PCA), which is the finite-dimensional version of the Karhunen-Lo\`{e}ve expansion \cite{loeve1978} where one seeks for eigenfunctions instead of eigenvectors. The expansion is yet another alternative to obtain $\vZ(\o)$ when general Gaussian processes with known covariance functions are present in the model \cite{maitre2010, xiu2010}.
