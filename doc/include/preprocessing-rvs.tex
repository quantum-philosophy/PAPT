As mentioned in \sref{uncertain-parameters}, $\vU(\o)$ should be preprocessed to extract a set of mutually independent r.v.'s $\vZ(\o)$. It is generally accepted that the variations of the channel length are distributed normally \cite{juan2011, juan2012, srivastava2010}, i.e., $\vU(\o) \sim \normal{\vZero}{\mCov_\vU}$; therefore, an appropriate linear transformation can be employed. Since any covariance matrix is necessarily a real, symmetric matrix, it admits the eigenvalue factorization \cite{press2007} as $\mCov_\vU = \m{V} \m{\Lambda} \m{V}^T$ where $\m{V}$ and $\m{\Lambda}$ are an orthogonal matrix of the eigenvectors and a diagonal matrix of the eigenvalues of $\mCov_\vU$, respectively. Consequently, $\vU(\o)$ can be decomposed as $\vU(\o) = \m{V} \m{\Lambda}^\ifrac{1}{2} \vZ(\o) = \oInvTransform{\vZ(\o)}$ where $\vZ(\o) \sim \normal{\vZero}{\mI}$ is the desired vector of centered, normalized, and mutually independent uncertain parameters, and $\oInvTransform{\idot}$ is the corresponding inverse transformation (see \sref{uncertain-parameters}). The total number of r.v.'s is $\vars = \cores + 1$.

The number of stochastic dimensions $\vars$ directly affects the computational cost of a PC expansion as it is seen in \eref{pc-terms}. Therefore, one should consider a possibility of model order reduction before constructing PC expansions. The intuition is that, due to the existing correlations between r.v.'s, some of them can be harmlessly replaced by linear combinations of the rest. One way to reveal these redundancies is to analyze the eigenvalues $\lambda_i$ found in $\m{\Lambda}$. Assume $\lambda_i$, $\forall i$, are arranged in a non-increasing order and let $\bar{\lambda}_i = \lambda_i / \sum_j \lambda_j$. Gradually summing up the arranged and normalized eigenvalues $\bar{\lambda}_i$, we can identify a subset of them, which has the cumulative sum greater than a certain threshold $\Lth$. When $\Lth$ is sufficiently high (close to 1), the rest of the eigenvalues and the corresponding eigenvectors can be dropped as being insignificant reducing the number of stochastic dimensions $\vars$.

The procedure described here is known as the principal component analysis (PCA), which is the finite-dimensional version of the Karhunen-Lo\`{e}ve expansion; the later can be employed construct an orthogonal expansion when the covariance function of the process is known \cite{maitre2010, xiu2010}.
