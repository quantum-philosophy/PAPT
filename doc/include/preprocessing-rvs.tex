As mentioned in \sref{uncertain-parameters}, the uncertain parameters should be preprocessed to extract a set of mutually independent r.v.'s \cite{xiu2009}. To this end, denote $\vlLeff(\o) = \vec{\lLeff_1(\o), \dotsc, \lLeff_\cores(\o)} \in \real^\cores$ and $\oCov{\vlLeff(\o)} = \mCov_\vlLeff$. (The covariance matrix $\mCov_\vlLeff$ in our example is generated according to \eref{covariance}.) It is generally accepted that the uncertainties of the channel length caused by the process variation are distributed normally \cite{srivastava2010, liu2007, juan2012}, therefore, $\gLeff(\o) \sim \normal{0}{\var_\gLeff}$ and $\vlLeff(\o) \sim \normal{\vZero}{\mCov_\vlLeff}$.

Since any covariance matrix is necessarily a real, symmetric matrix, it admits the eigenvalue factorization \cite{press2007} in the form $\mCov_\vlLeff = \m{V}_{\mCov_\vlLeff} \m{\Lambda}_{\mCov_\vlLeff} \m{V}_{\mCov_\vlLeff}^T$ where $\m{V}_{\mCov_\vlLeff}$ and $\m{\Lambda}_{\mCov_\vlLeff}$ are an orthogonal matrix of the eigenvectors and a diagonal matrix of the eigenvalues of $\mCov_\vlLeff$, respectively. Consequently, $\vlLeff(\o)$ can be normalized as $\vlLeff(\o) = \m{V}_{\mCov_\vlLeff}^T \m{\Lambda}_{\mCov_\vlLeff}^\ifrac{1}{2} \vlZ(\o)$ where $\vlZ(\o) \sim \normal{\vZero}{\mI}$. The components of $\vlZ(\o)$ are uncorrelated and, since Gaussian, mutually independent. The global variation $\gLeff(\o)$, without loss of generality, is assumed to be uncorrelated with respect to each local r.v. $\lLeff_i(\o)$, hence, it is also independent from $\lLeff_i(\o)$. $\gLeff(\o)$ is normalized as $\gLeff(\o) = \std_\gLeff \gZ(\o)$ where $\gZ(\o) \sim \normal{0}{1}$. The later is appended to $\vlZ(\o)$ forming the desired vector of mutually independent and normalized r.v.'s $\vZ(\o) \in \real^{\cores + 1}$.

The number of stochastic dimensions $\vars$, which so far is $(\cores + 1)$, essentially effects computational costs, therefore, we should consider the possibility of model reduction at this stage as well. The intuition is that, due to the existing correlations between r.v.'s, some of them can be harmlessly replaced by linear combinations of the rest. The diagonal matrix of eigenvalues $\m{\Lambda}_{\mCov_\vlLeff}$ reveals these redundancies by having a set of relatively small eigenvalues, which can be dropped reducing the number of stochastic dimensions of the problem.

The whole described procedure is known as the Principal Component Analysis (PCA), and it is a discrete version of the Karhunen-Lo\`{e}ve expansion \cite{loeve1978}. The later can be successfully employed for our purposes for general Gaussian processes with known covariance functions.
