As mentioned in \sref{uncertain-parameters}, the uncertain parameters $\vU(\o)$ should be preprocessed to extract a set of mutually independent r.v.'s $\vZ(\o)$. It is generally accepted that the uncertainties of the channel length caused by the process variation are distributed normally \cite{juan2011, juan2012, srivastava2010}, i.e., $\vU(\o) \sim \normal{\vZero}{\mCov_\vU}$, therefore, according to the strategy outlined in \sref{uncertain-parameters}, an appropriate linear transformation can be employed.

For conveniece, we extend the correlation matrix of the local r.v.'s $\lLeff_i(\o)$ given in \eref{correlation-matrix} by one dimension to pack the global r.v. $\gLeff(\o)$ together with the local ones. Since global variations are typically assumed to be uncorrelated with respect to local r.v.'s, which has no loss of generality, we need to append a zero row and a zero column with the only one non-zero exception on the diagonal equal to one. Taking into account the deviations of the r.v.'s, the covariance matrix $\mCov_\vU$ can be formed. Since any covariance matrix is necessarily a real, symmetric matrix, it admits the eigenvalue factorization \cite{press2007} as $\mCov_\vU = \m{V} \m{\Lambda} \m{V}^T$ where $\m{V}$ and $\m{\Lambda}$ are an orthogonal matrix of the eigenvectors and a diagonal matrix of the eigenvalues of $\mCov_\vU$, respectively. Consequently, $\vU(\o)$ can be decomposed as $\vU(\o) = \m{V} \m{\Lambda}^\ifrac{1}{2} \vZ(\o) = \oInvTransform{\vZ(\o)}$ where $\vZ(\o) \sim \normal{\vZero}{\mI}$ is the desired vector of centered, normalized, and mutually independent uncertain parameters, and $\oInvTransform{\idot}$ is the corresponding inverse transformation (see \sref{uncertain-parameters}). The total number of r.v.'s is $\vars = \cores + 1$.

The number of stochastic dimensions $\vars$ directly affects the overall computational cost since it increases the number of terms $\pcterms$ (\eref{pc-terms}) in the PC expansion and the difficulty of inner products for the corresponding coefficients (\eref{pc-coefficients}). Therefore, we should consider a possibility of model order reduction before constructing PC expansions. The intuition is that, due to the existing correlations between r.v.'s, some of them can be harmlessly replaced by linear combinations of the rest. One way to reveal these redundancies is to analyze the diagonal matrix of eigenvalues $\m{\Lambda} = \diag{ \lambda_i }$, which we obtained in the previous paragraph. Assume $\lambda_i$, $\forall i$, are arranged in a non-increasing order and let $\bar{\lambda}_i = \lambda_i / \sum_j \lambda_j$. Gradually summing up the arranged and normalized eigenvalues $\bar{\lambda}_i$, we can identify a subset of them, which has the cumulative sum greater than a certain threshold $\Lth$. When $\Lth$ is sufficiently high (close to 1), the rest of the eigenvalues and the corresponding eigenvectors can be dropped as being insignificant reducing the number of stochastic dimensions $\vars$.

The whole procedure described in this subsection is known as the principal component analysis (PCA), which is the finite-dimensional version of the Karhunen-Lo\`{e}ve expansion \cite{loeve1978} where one seeks for eigenfunctions instead of eigenvectors. The expansion is yet another alternative to obtain $\vZ(\o)$ when general Gaussian processes with known covariance functions are present in the model \cite{maitre2010, xiu2010}.
