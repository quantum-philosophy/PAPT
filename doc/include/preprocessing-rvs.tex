As mentioned in \sref{uncertain-parameters}, the uncertain parameters $\vU(\o)$ should be preprocessed to extract a set of mutually independent r.v.'s $\vZ(\o)$. It is generally accepted that the uncertainties of the channel length caused by the process variation are distributed normally \cite{srivastava2010, liu2007, juan2012}, i.e., $\vU(\o) \sim \normal{\vZero}{\mCov_\vU}$, therefore, according to the strategy outlined in \sref{uncertain-parameters}, an appropriate linear transformation can be employed.

Global variations are typically assumed to be uncorrelated with respect to local variations, which has no loss of generality. Therefore, we can treat these two parts separately. $\gLeff(\o)$ can be simply normalized as $\gLeff(\o) = \std_\gLeff \gZ(\o)$ where $\gZ(\o) \sim \normal{0}{1}$. For the local variations, denote $\vlLeff(\o) = \vec{\lLeff_1(\o), \dotsc, \lLeff_\cores(\o)} \in \real^\cores$ and $\oCov{\vlLeff(\o)} = \mCov_\vlLeff$. (The covariance matrix $\mCov_\vlLeff$ in our example is generated according to \eref{covariance}.) Since any covariance matrix is necessarily a real, symmetric matrix, it admits the eigenvalue factorization \cite{press2007} in the form $\mCov_\vlLeff = \m{V} \m{\Lambda} \m{V}^T$ where $\m{V}$ and $\m{\Lambda}$ are an orthogonal matrix of the eigenvectors and a diagonal matrix of the eigenvalues of $\mCov_\vlLeff$, respectively. Consequently, $\vlLeff(\o)$ can be decomposed as $\vlLeff(\o) = \m{V} \m{\Lambda}^\ifrac{1}{2} \vlZ(\o)$ where $\vlZ(\o) \sim \normal{\vZero}{\mI}$. The components of $\vlZ(\o)$ are uncorrelated and, since Gaussian, independent. Finally, $\gZ(\o)$ is appended to $\vlZ(\o)$ forming the desired vector of mutually independent and normalized uncertain parameters $\vZ(\o) \in \real^{\vars = \cores + 1}$.

The number of stochastic dimensions $\vars$, which so far is $(\cores + 1)$, directly affects the computational costs, therefore, we should consider a possibility of model reduction at this stage as well. The intuition is that, due to the existing correlations between r.v.'s, some of them can be harmlessly replaced by linear combinations of the rest. The diagonal matrix of eigenvalues $\m{\Lambda}$ reveals these redundancies by having a set of relatively small eigenvalues, which can be dropped reducing the number of stochastic dimensions of the problem.

The whole procedure described in this subsection is known as the Principal Component Analysis (PCA), and it is a discrete version of the Karhunen-Lo\`{e}ve expansion \cite{loeve1978}, in which one seeks for eigenfunctions instead of eigenvectors. The expansion is yet another alternative to obtain $\vZ(\o)$ when general Gaussian processes with known covariance functions are present in the model \cite{xiu2010}.
