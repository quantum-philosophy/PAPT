At \stage{1}, $\vU(\o)$ should be preprocessed in order to extract a set of mutually independent \rvs, $\vZ(\o)$. It is generally accepted that the variations of the channel length have Gaussian distributions \cite{srivastava2010, juan2011, juan2012}; therefore, we assume that $\vU(\o)$ is a Gaussian vector, \ie, $\vU(\o) \sim \normal(\vZero, \mCov_\vU)$. Consequently, an appropriate linear transformation can be employed. More specifically, the procedure described in this section is known as the principal component analysis (PCA), which is the finite-dimensional version of the KL expansion.

Since any covariance matrix is necessarily a real, symmetric matrix, it admits the eigenvalue decomposition \cite{press2007} $\mCov_\vU = \m{V} \m{\Lambda} \m{V}^T$ where $\m{V}$ and $\m{\Lambda}$ are an orthogonal matrix of the eigenvectors and a diagonal matrix of the eigenvalues of $\mCov_\vU$, respectively. Consequently, $\vU(\o)$ can be factorized as $\vU(\o) = \m{V} \: \m{\Lambda}^{1/2} \: \vZ(\o) = \oTransform{\vZ(\o)}$ where $\vZ(\o) \sim \normal(\vZero, \mI)$ is the desired vector of centered, normalized, and mutually independent uncertain parameters, and $\oTransform$ is the corresponding transformation (see the discussion in \sref{uncertain-parameters}).

The number of stochastic dimensions $\nvars$, which so far is $\nprocs + 1$, directly impacts the computational cost of PC expansions as it is discussed in the appendix, \aref{polynomial-chaos}. Therefore, one should consider a possibility of model order reduction before constructing PC expansions. The intuition is that, due to the existing correlations between \rvs, some of them can be harmlessly replaced by linear combinations of the rest. One way to reveal these redundancies is to analyze the eigenvalues $\lambda_i$ found in $\m{\Lambda}$ given above. Assume $\lambda_i$, $\forall i$, are arranged in a non-increasing order and let $\tilde{\lambda}_i = \lambda_i / \sum_j \lambda_j$. Gradually summing up the arranged and normalized eigenvalues $\tilde{\lambda}_i$, we can identify a subset of them, which has the cumulative sum greater than a certain threshold $\Lth$. When $\Lth$ is sufficiently high (close to one), the rest of the eigenvalues and their eigenvectors can be dropped as being insignificant, reducing the number of stochastic dimensions $\nvars$. The results of the algorithm are reported in \sref{experimental-results}.
