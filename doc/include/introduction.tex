Over the last decade, the concept of uncertainty quantification (UQ) has become central for a wide range of application areas \cite{xiu2010, eldred2009}. The primary target of UQ is characterization of the output of systems that exhibit non-deterministic behavior due to the presence of uncertainties of some kind. A multiprocessor platform is a prominent example of such a system, wherein the variability originates from, \eg, the semiconductor manufacturing process and operating environment. In particular, the uncertainty impacts power and, consequently, temperature, which are among the main concerns of multiprocessor system designs. As an example, consider a quad-core architecture subjected to uncertainty of the parameters that affect the leakage current.\footnote{The experimantal setup is thoroughly explained in \sref{experimental-results}.} Assume, first, that these parameters have nominal values. We can then simulate the system under a certain dynamic power profile in order to observe the corresponding temperature. The result, labeled as ``Nominal'', is depicted in \fref{motivation-curve} where, for clarity, only two curves, corresponding to two processors, are presented (the two bottom blue lines). It can be seen that the temperature is always below $70^{\circ}$C. Now, let us assume a mild deviation of the parameters from the nominal values and run the simulation once again. The result is the ``Mild'' curves in \fref{motivation-curve} (the two middle orange lines); the maximal temperature is already $80^{\circ}$C. Finally, we repeat the experiment considering a severe deviation of the leakage parameters and observe the curves labeled as ``Severe'' in \fref{motivation-curve} (the two top green lines); the maximal temperature is almost $110^{\circ}$C. Imagine that the designer, when tuning the solution with the maximal temperature constraint of $80^\circ$C, was guided exclusively by the nominal parameters. In this case, even with mild deviations, the circuits might be burnt. The amount of burnt circuits depends on the statistical distribution of deviations. As we see, the uncertainty has to be addressed in order to pursue efficiency and robustness. Nevertheless, one can observe that the majority of the literature, which involves system-level power-temperature analysis (PTA), ignores these important aspects, \eg, \cite{rao2009, rai2011, thiele2011, ukhov2012}; therefore, the reliability of the corresponding results is questionable in practice.

To overcome the limitations of deterministic PTA, a number of stochastic PTA techniques have been recently introduced. A solely power-targeted but temperature-aware solution is proposed in \cite{chandra2010}, which employs Monte Carlo (MC) simulations to account for process and environmental variations of multiprocessor systems. A learning-based approach is presented in \cite{juan2011} to estimate the maximal temperature under the steady-state condition and variability of the leakage current. Leakage is also considered in \cite{juan2012}, where a statistical model of the steady-state temperature based on Gaussian distributions is derived. None of the aforementioned techniques attempts to perform the stochastic \emph{transient} PTA and to compute the evolving-in-time probability distribution of temperature. However, such transient curves are of practical importance. First of all, certain procedures cannot be undertaken without the knowledge of time-dependent temperature variations, \eg, the reliability optimization based on the thermal-cycling fatigue \cite{ukhov2012}. Secondly, the constant steady-state temperature assumption, considered, \eg, in \cite{juan2011, juan2012}, can rarely be justified since power profiles are not invariant in reality. Thirdly, the frequently made assumption that power and/or temperature follow \apriori\ known probability distributions---for instance, Gaussian and log-normal distributions are popular choices, as in \cite{juan2012, srivastava2010}---is not realistic due to (a) the strict nonlinearities between the process parameters, power, and temperature; (b) the nonlinear interdependency of temperature and the leakage power \cite{liu2007}. To illustrate this, we simulated the previously mentioned example $10^4$ times with respect to the distribution of the parameters taken from the literature (discussed in \sref{illustrative-example}), and performed kernel density estimation of probability density functions (\pdfs) of the temperature of the four processors at the middle of the time span shown in \fref{motivation-curve}. The results are depicted in \fref{motivation-pdf}, and it can be seen that they are neither Gaussian nor log-normal.\footnote{To confirm this, we applied the Jarque-Bera test \cite{juan2012} of normality to the original data as well as to the data processed by the Box-Cox transformation \cite{juan2012}. In both cases, the null hypothesis, \ie, the data are from a Gaussian distribution, was rejected at the 5\% significance level.} To conclude, the present models of uncertainty and the stochastic PTA techniques for multiprocessor system design are restricted in use due to one or several of the following traits: based on MC simulations (potentially slow as we shall discuss shortly) \cite{chandra2010}, limited to power analysis \cite{chandra2010}, limited to the assumption of the constant steady-state temperature \cite{juan2011, juan2012}, exclusive focus on the maximal temperature \cite{juan2011}, \apriori\ chosen distributions of power and temperature \cite{juan2012, srivastava2010}. Consequently, there is a lack of flexible stochastic PTA techniques, which we aim to fulfill.

\input{include/assets/motivation.tex}
A straightforward approach, which is mentioned earlier in the context of \cite{chandra2010}, to analyze a stochastic system is the MC sampling coupled with a deterministic simulator. The major problem with the MC sampling is the low rate of convergence, \eg, the mean value converges as $\mcsamples^{-\ifrac{1}{2}}$ where $\mcsamples$ is the number of samples \cite{xiu2010, maitre2010}. This means that, in order to get an additional decimal point of accuracy, one has to obtain hundred times more samples. Each such sample implies a complete realization of the whole system; therefore, MC-based methods are typically slow and often infeasible since the number of needed simulations can be extremely large in order to obtain reliable estimates \cite{diaz-emparanza2002}.

Attractive alternatives to the MC sampling are spectral methods \cite{xiu2010, maitre2010} featuring much faster convergence properties. One such method is applied in this paper, namely, the generalized polynomial chaos (PC) \cite{xiu2002}, which is the current state of the art in numerical analysis of stochastic systems. The popularity of PC is dictated by its applicability to a wide range of UQ problems and, more importantly, by its ability to construct easy-to-analyze representations of system responses to stochastic inputs. PC is commonly accompanied by another spectral method known as the Karhunen-Lo\`{e}ve (KL) expansion \cite{ghanem1991}, which we also utilize. KL is useful as a technique for reduction of the number of uncertain parameters and, hence, of the resulting computational costs. In \cite{shen2009}, PC based on Hermite polynomials is employed to estimate the full-chip leakage; the KL expansion is used in \cite{bhardwaj2006} to calculate the leakage current of electrical circuits; an analysis of the voltage response of power grids is carried out in \cite{ghanta2006}, where the PC and KL expansions are jointly utilized.

The contribution of this paper is in the following: we develop, for the first time, a framework for UQ of transient power and temperature variations of multiprocessor systems that depend on a set of uncertain parameters. The framework is flexible in modeling diverse probability distributions, specified by the user, of the parameters, and there are no assumptions on the distributions of the resulting power and temperature traces as these distributions are unlikely to be known \apriori. Furthermore, the proposed technique is capable of capturing arbitrary joint effects of the uncertain parameters on the system since the parameters are introduced into the model as a ``black'' box, which is also defined by the user. Based on a nominal dynamic power profile, our technique produces the corresponding stochastic power and temperature profiles given as polynomials of \rvs; the expressions are straightforward to be further analyzed. The framework is illustrated on one of the most important parameters affected by process variation: the subthreshold leakage current. Note, however, that our approach is not bounded to any particular source of variability and, apart from the process-related variations, can be applied to other uncertainties such as those due to environment, \ie, fluctuation of the supply voltage, ambient temperature, \etc

\input{include/assets/algorithm.tex}
The reminder of the paper is organized as follows. In \sref{preliminaries}, we introduce the architecture model, which we shall consider, and formulate the objective of our study. The proposed framework is presented in \sref{proposed-framework}, wherein a rather general description is given. A particular application of our technique is discussed in \sref{illustrative-example}, and the corresponding results are compared with MC simulations in \sref{experimental-results}. \sref{conclusion} concludes the paper. The work contains a set of supplementary materials with in-depth discussions on certain aspects of our approach.
