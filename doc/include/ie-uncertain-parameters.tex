As mentioned in \sref{uncertain-parameters}, $\vU(\o)$ should be preprocessed in order to extract a set of mutually independent \rvs, $\vZ(\o)$. It is generally accepted that the variations of the channel length are distributed normally \cite{juan2011, juan2012, srivastava2010}, i.e., $\vU(\o) \sim \normal{\vZero}{\mCov_\vU}$; therefore, an appropriate linear transformation can be employed. More specifically, the procedure described in this section is known as the principal component analysis (PCA), which is the finite-dimensional version of the KL expansion.

Since any covariance matrix is necessarily a real, symmetric matrix, it admits the eigenvalue factorization \cite{press2007} as $\mCov_\vU = \m{V} \m{\Lambda} \m{V}^T$ where $\m{V}$ and $\m{\Lambda}$ are an orthogonal matrix of the eigenvectors and a diagonal matrix of the eigenvalues of $\mCov_\vU$, respectively. Consequently, $\vU(\o)$ can be decomposed as $\vU(\o) = \m{V} \m{\Lambda}^\ifrac{1}{2} \vZ(\o) = \oInvTransform{\vZ(\o)}$ where $\vZ(\o) \sim \normal{\vZero}{\mI}$ is the desired vector of centered, normalized, and mutually independent uncertain parameters, and $\oInvTransform{\idot}$ is the corresponding inverse transformation (see \sref{uncertain-parameters}).

The number of stochastic dimensions $\vars$, which so far is $\cores + 1$, directly impacts the computational cost of a PC expansion as it is seen in \eref{pc-terms}. Therefore, one should consider a possibility of model order reduction before constructing PC expansions. The intuition is that, due to the existing correlations between \rvs, some of them can be harmlessly replaced by linear combinations of the rest. One way to reveal these redundancies is to analyze the eigenvalues $\lambda_i$ found in $\m{\Lambda}$. Assume $\lambda_i$, $\forall i$, are arranged in a non-increasing order and let $\bar{\lambda}_i = \lambda_i / \sum_j \lambda_j$. Gradually summing up the arranged and normalized eigenvalues $\bar{\lambda}_i$, we can identify a subset of them, which has the cumulative sum greater than a certain threshold $\Lth$. When $\Lth$ is sufficiently high (close to one), the rest of the eigenvalues and their eigenvectors can be dropped as being insignificant, reducing the number of stochastic dimensions $\vars$. The results of the algorithm are reported in \sref{experimental-results}.
