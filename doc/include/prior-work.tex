Since the appearance of the first digital computers in 1940s, Monte Carlo (MC) sampling remains one of the most well-known and widely used methods for the analysis of stochastic systems.
The reason for this popularity lies in the ease of implementation, in the independence of the stochastic dimensionality of the considered problems, and in the fact that the quantities estimated using MC simulations asymptotically approach the true values (the law of large numbers).
The crucial problem with MC sampling, however, is the low rate of convergence: the error decreases at the order of $\nsamples^{-1/2}$ where $\nsamples$ is the number of samples.\footnote{There are other sampling techniques that have better convergence rates than the one of the classical MC sampling, \eg, quasi-MC sampling; however, due to additional restrictions, their applicability is often limited \cite{xiu2010}.}
This means that, in order to get an additional decimal point of accuracy, one has to obtain hundred times more samples.
Each such sample implies a complete realization of the whole system, which renders MC-based methods slow and often infeasible since the needed number of simulations can be extremely large \cite{diaz-emparanza2002}.

In order to overcome the limitations of deterministic power-temperature analysis (PTA) and, at the same time, to completely eliminate or, at least, mitigate the costs associated with MC sampling, a number of alternative stochastic PTA techniques have been recently introduced.
Due to the fact that the leakage component of the power dissipation is influenced by process variation the most \cite{chandrakasan2001, srivastava2010, juan2011, juan2012}, the techniques discussed below primarily focus on the variability of leakage.

A solely power-targeted but temperature-aware solution is proposed in \cite{chandra2010} wherein the driving force of the analysis is MC sampling with partially precomputed data.
A learning-based approach is presented in \cite{juan2011} to estimate the maximal temperature under the steady-state condition.
Temperature-related issues originating from process variation are also considered in \cite{juan2012} where a statistical model of the steady-state temperature based on Gaussian distributions is derived.
A statistical steady-state temperature simulator is developed in \cite{huang2009} using polynomial chaos (PC) expansions and the Karhunen-Lo\`{e}ve (KL) decomposition \cite{xiu2010, ghanem1991}.
A KL-aided stochastic collocation \cite{xiu2010} approach to steady-state temperature analysis is presented in \cite{lee2013}.
In \cite{shen2009}, PC expansions are employed to estimate the full-chip leakage power.
The KL decomposition is utilized in \cite{bhardwaj2006} for leakage calculations.
In \cite{bhardwaj2008}, the total leakage is quantified using the PC and KL methods.
The same combination of tools is employed in \cite{vrudhula2006} and \cite{ghanta2006} to analyze the response of interconnect networks and power grids, respectively, under process variation.

The last five of the aforementioned techniques, \ie, \cite{shen2009, bhardwaj2006, bhardwaj2008, vrudhula2006, ghanta2006}, perform only stochastic power analysis and ignore the interdependence between leakage and temperature.
The others are temperature-related approaches, but none of them attempts to tackle stochastic transient PTA and to compute the evolving-in-time probability distributions of temperature.
However, such transient curves are of practical importance.
First of all, certain procedures cannot be undertaken without the knowledge of the time-dependent temperature variations, \eg, reliability optimization based on the thermal-cycling fatigue \cite{ukhov2012}.
Secondly, the constant steady-state temperature assumption, considered, \eg, in \cite{juan2011, juan2012, huang2009, lee2013}, can rarely be justified since power profiles are not invariant in reality.
In addition, one can frequently encounter the assumption that power and/or temperature follow \apriori\ known probability distributions, for instance, Gaussian and log-normal distributions are popular choices as in \cite{srivastava2010, juan2012, bhardwaj2006}.
However, this assumption often fails in practice (also noted in \cite{juan2012} regarding the normality of the leakage power) due to: (a) the strict nonlinearities between the process-related parameters, power, and temperature; (b) the nonlinear interdependency of temperature and the leakage power \cite{liu2007}.
To illustrate this, we simulated the example given in \sref{introduction} $10^4$ times assuming the widespread Gaussian model for the variability of the effective channel length; the rest of the experimental setup was configured as it will be described in \sref{illustrative-example} and \sref{experimental-results}.
Then we applied the Jarque-Bera test of normality to the collected data (temperature) directly as well as after processing them with the log transformation.
The null hypothesis that the data are from an unspecified Gaussian distribution was firmly rejected in both cases at the significance level of 5\%.
Therefore, the two distributions are neither Gaussian nor log-normal, which can also be seen in \fref{experimental-results-pdf} described in the experimental results, \sref{experimental-results}.

To conclude, the prior stochastic PTA techniques for electronic system design are restricted in use due to one or several of the following traits: based on MC simulations (potentially slow) \cite{chandra2010}, limited to power analysis \cite{chandra2010, shen2009, bhardwaj2006, bhardwaj2008, vrudhula2006, ghanta2006}, ignoring the leakage-temperature interplay \cite{huang2009, shen2009, bhardwaj2006, bhardwaj2008, vrudhula2006, ghanta2006}, limited to the assumption of the constant steady-state temperature \cite{juan2011, juan2012, huang2009, lee2013}, exclusive focus on the maximal temperature \cite{juan2011}, and \apriori\ chosen distributions of power and temperature \cite{srivastava2010, juan2012, bhardwaj2006}.
Consequently, there is a lack of flexible stochastic PTA techniques, which we aim to eliminate.
